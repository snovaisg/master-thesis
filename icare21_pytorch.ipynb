{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8419e1-b7d4-4005-a481-4326072e3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af9ac95-fa01-4484-835a-7d78b8634268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rnn_utils import split_dataset, MYCOLLATE\n",
    "from rnn_utils import ICareDataset_fast, ICareCOLLATE_fast\n",
    "from rnn_utils import RNN, train_one_epoch, train_one_epochV2, eval_model, compute_loss, get_prediction_thresholds, outs2df,compute_metrics\n",
    "from rnn_utils import gen_mask_padded_loss\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from config import Settings; settings = Settings()\n",
    "\n",
    "from ICDMappings import ICDMappings\n",
    "icdmap = ICDMappings()\n",
    "\n",
    "import wandb\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de9a85-c2c4-4580-ae46-6c308b068004",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93403715-725c-4796-abf2-a99eb3b8b06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7fbf559bf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducibility\n",
    "seed = settings.random_seed\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf275b-ac37-415b-b590-5768ee07df85",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56824288-de03-45a9-b280-c293be85481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = 'ccs'\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a1efd3d-c806-4d3a-808a-f8fc8b54bf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing each patient\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa71f0b7833a4bf0a1097e838e872219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2875"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccs_universe = list(icdmap.icd9_3toccs.data.keys())\n",
    "dataset_folder = '/home/debian/Simao/master-thesis/data/model_ready_dataset/icare2021_diag_A301'\n",
    "dataset = ICareDataset_fast(os.path.join(dataset_folder,'dataset.json'),\n",
    "                            ccs_universe,\n",
    "                            grouping\n",
    "                          )\n",
    "\n",
    "train_dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=ICareCOLLATE_fast(),\n",
    "                            sampler=RandomSampler(dataset.train_indices)\n",
    "                           )\n",
    "val_dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=ICareCOLLATE_fast(),\n",
    "                            sampler=RandomSampler(dataset.val_indices)\n",
    "                           )\n",
    "test_dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=ICareCOLLATE_fast(),\n",
    "                            sampler=RandomSampler(dataset.test_indices)\n",
    "                           )\n",
    "\n",
    "# Nº batches\n",
    "len(train_dataloader)\n",
    "len(val_dataloader)\n",
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "187f86bc-60b7-413a-b8b7-c3f32a0d347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f9bb0facde4237834ce6aca7707bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0     0.23\n",
       "1     0.18\n",
       "2     0.13\n",
       "3     0.10\n",
       "4     0.07\n",
       "5     0.06\n",
       "6     0.04\n",
       "7     0.03\n",
       "8     0.03\n",
       "9     0.02\n",
       "10    0.02\n",
       "11    0.01\n",
       "12    0.01\n",
       "13    0.01\n",
       "14    0.01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Distribution of target size (nº of diagnostics in target window)')\n",
    "all_lengths = {}\n",
    "for e in tqdm(dataset.raw_data):\n",
    "    lengths = [len(i) for i in dataset.raw_data[e]['ccs']['targets']]\n",
    "    for l in lengths:\n",
    "        if l not in all_lengths:\n",
    "            all_lengths[l] = 1\n",
    "        else:\n",
    "            all_lengths[l] +=1\n",
    "            \n",
    "(pd.Series(all_lengths).sort_index() / pd.Series(all_lengths).sum())[:15].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d141a-1611-43c1-ad6f-0d05f36dac7e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dff6db4-d63e-490a-86e4-48116010366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = input_size = next(iter(train_dataloader))['target_sequences']['sequence'].shape[2]\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adfcca93-952d-4fbe-879e-7f9399b8596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 8\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'hidden_size':[100,150],\n",
    "    'num_layers':[1],\n",
    "    'lr':[0.01,0.02],\n",
    "    'model':['rnn','gru']\n",
    "    \n",
    "}\n",
    "meta_parameters = {\n",
    "    'epochs':1\n",
    "}\n",
    "\n",
    "params = ParameterGrid(hyperparameters)\n",
    "print(f'params:',len(params))\n",
    "\n",
    "#random_params = ParameterSampler(params.param_grid,n_iter=len(params)-1,random_state=231)\n",
    "#next(iter(random_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2b7cf-a16a-4faf-82fb-3e7d7cecec4e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97768719-dc3f-4561-8e2f-5ed7cf167462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence, pack_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd3914-1d39-4452-a12f-73f2f18ebb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61cb5bc-9dbd-4d48-8154-67c5ed69a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import recall,precision,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af73c7-3eeb-419f-a5f5-2592da6c380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bcdbf3-6388-404d-acef-8a0077b21522",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_set = next(iter(params))\n",
    "config = param_set\n",
    "model = RNN(input_size=input_size,\n",
    "              hidden_size=config['hidden_size'],\n",
    "              num_layers=config['num_layers'],\n",
    "              n_labels=n_labels,\n",
    "              model=config['model'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68cc99c-4d2f-4cd4-8c7d-a4b94ec5201a",
   "metadata": {},
   "source": [
    "# Improved masked stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8c5de2d9-a5b7-46d5-aa47-3a0000650f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mask_padded_loss(lengths,loss_shape):\n",
    "    \"\"\"\n",
    "    This method creates a mask to later perform loss.masked_fill_(mask,0)\n",
    "    \n",
    "    Note: this method is called at each batch so it has been optimized to some extent\n",
    "    sacrificing some readibility. Hence it may be criptic to understand everything.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lengths: list, shape = (batch_size,)\n",
    "        list with the actual length of each sequence in the batch\n",
    "    \n",
    "    loss_shape: tuple/list, shape=(batch_size, max_seq_length, n_labels)\n",
    "        shape of the loss tensor on a given batch\n",
    "    \"\"\"\n",
    "    idx = pd.IndexSlice\n",
    "    # i.e. [ (pos_in_batch, seq_size), ...]\n",
    "    # e.g. imagine batch of two sequences. first has size 2 and second size 6. we get [(0,2),(1,6)]\n",
    "    seq_size_per_seq = list(zip(range(0,len(lengths)),lengths.numpy()))\n",
    "\n",
    "    # i.e. [ (pos_in_batch,seq_index),(pos_in_batch,seq_index),...]\n",
    "    # e.g. imagine batch of two seqs. first has size 2, second has size 1. produces: [[(0,0),(0,1)],[(1,0)]]\n",
    "    real_seq_pos_per_seq = [list(zip([a[0]]*a[1],range(0,a[1]))) for a in seq_size_per_seq]\n",
    "    # just flattens the previous list.\n",
    "    # i.e. (taking the previous example) produces: [(0,0),(0,1),(1,0)]\n",
    "    real_seq_pos_per_seq = [item for seq in real_seq_pos_per_seq for item in seq] \n",
    "\n",
    "    # create a mask that initially has everything as True\n",
    "    res = (pd.DataFrame(np.ones(shape=(loss_shape[0]*loss_shape[1],loss_shape[2])))\n",
    "           .assign(seq=np.array([[seq] * loss_shape[1] for seq in range(len(lengths))]).reshape((-1,1)),\n",
    "                   index=list(range(0,loss_shape[1]))*loss_shape[0]\n",
    "                  )\n",
    "           .set_index(['seq','index']) # index is mean to help in the .loc after this cascade\n",
    "           .astype(bool) # all values of dataframe are set to False now.\n",
    "          )\n",
    "    \n",
    "    # set to False the values we don't want to change (aka: values that are not paddings)\n",
    "    res.loc[idx[real_seq_pos_per_seq],:] = False\n",
    "\n",
    "    # stack from (batch_size*max_seq_length,n_labels) to (batch_size,max_seq_length,n_labels)\n",
    "    mask = torch.tensor(res.to_numpy().reshape(loss_shape))\n",
    "    \n",
    "    # now the mask has the same shape as the loss and ready to be applied on torch.masked_fill_\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65737532-8451-4ac1-b5f7-821a893e4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a164ac6a-431f-4823-9af7-f8525b52b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    history_sequences, target_sequences = batch['train_sequences']['sequence'], batch['target_sequences']['sequence']\n",
    "    \n",
    "    outs = model(history_sequences,target_sequences)\n",
    "    \n",
    "    sequences,lengths = pad_packed_sequence(history_sequences,batch_first=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f86e7e57-78bd-4b88-9294-951f5911b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(40)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 24, 32,  5,  5,  5, 10,  4,  7,  6,  3,  1, 13,  5,  5,  7,  3,  2,\n",
       "         5,  7,  3,  5,  8,  4,  7,  2,  3, 12,  6,  4,  2, 19,  2,  8,  5,  4,\n",
       "        40,  4,  9,  2, 14, 13,  2,  2,  2,  4, 19, 13,  1,  4, 13,  2, 12,  2,\n",
       "         7, 19,  2,  1,  5,  3,  5,  4,  6,  3])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max()\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b99772cb-20b3-4655-9e17-2268167c444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size_per_seq = list(zip(range(0,len(lengths)),lengths.numpy()))\n",
    "real_seq_pos_per_seq = [list(zip([a[0]]*a[1],range(0,a[1]))) for a in seq_size_per_seq]\n",
    "# just flattens the previous list.\n",
    "# i.e. (taking the previous example) produces: [(0,0),(0,1),(1,0)]\n",
    "real_seq_pos_per_seq = [item for seq in real_seq_pos_per_seq for item in seq] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "785a79c4-1aed-4602-8ea6-0222743143b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(size=(33,10,10))\n",
    "a[[0,1,5],:] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1759054-e487-4e40-82e5-490e47097448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "caa75f5a-3803-4470-8c4b-ade300b8597b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -8.3227,  -7.1623,  -6.1103,  ..., -11.4194,  -8.4300, -10.4651],\n",
       "        [ -7.6110,  -5.9118,  -5.0685,  ..., -12.3436,  -8.8899, -12.1421],\n",
       "        [ -8.0529,  -6.9389,  -5.7696,  ..., -13.1077,  -9.6372, -12.8298],\n",
       "        ...,\n",
       "        [ -0.1980,  -0.0790,  -0.1146,  ...,  -0.1642,  -0.0840,  -0.2897],\n",
       "        [ -0.1980,  -0.0790,  -0.1146,  ...,  -0.1642,  -0.0840,  -0.2897],\n",
       "        [ -0.1980,  -0.0790,  -0.1146,  ...,  -0.1642,  -0.0840,  -0.2897]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 440,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 680,\n",
       " 681,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 1000,\n",
       " 1001,\n",
       " 1040,\n",
       " 1041,\n",
       " 1042,\n",
       " 1080,\n",
       " 1081,\n",
       " 1082,\n",
       " 1083,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1087,\n",
       " 1088,\n",
       " 1089,\n",
       " 1090,\n",
       " 1091,\n",
       " 1120,\n",
       " 1121,\n",
       " 1122,\n",
       " 1123,\n",
       " 1124,\n",
       " 1125,\n",
       " 1160,\n",
       " 1161,\n",
       " 1162,\n",
       " 1163,\n",
       " 1200,\n",
       " 1201,\n",
       " 1240,\n",
       " 1241,\n",
       " 1242,\n",
       " 1243,\n",
       " 1244,\n",
       " 1245,\n",
       " 1246,\n",
       " 1247,\n",
       " 1248,\n",
       " 1249,\n",
       " 1250,\n",
       " 1251,\n",
       " 1252,\n",
       " 1253,\n",
       " 1254,\n",
       " 1255,\n",
       " 1256,\n",
       " 1257,\n",
       " 1258,\n",
       " 1280,\n",
       " 1281,\n",
       " 1320,\n",
       " 1321,\n",
       " 1322,\n",
       " 1323,\n",
       " 1324,\n",
       " 1325,\n",
       " 1326,\n",
       " 1327,\n",
       " 1360,\n",
       " 1361,\n",
       " 1362,\n",
       " 1363,\n",
       " 1364,\n",
       " 1400,\n",
       " 1401,\n",
       " 1402,\n",
       " 1403,\n",
       " 1440,\n",
       " 1441,\n",
       " 1442,\n",
       " 1443,\n",
       " 1444,\n",
       " 1445,\n",
       " 1446,\n",
       " 1447,\n",
       " 1448,\n",
       " 1449,\n",
       " 1450,\n",
       " 1451,\n",
       " 1452,\n",
       " 1453,\n",
       " 1454,\n",
       " 1455,\n",
       " 1456,\n",
       " 1457,\n",
       " 1458,\n",
       " 1459,\n",
       " 1460,\n",
       " 1461,\n",
       " 1462,\n",
       " 1463,\n",
       " 1464,\n",
       " 1465,\n",
       " 1466,\n",
       " 1467,\n",
       " 1468,\n",
       " 1469,\n",
       " 1470,\n",
       " 1471,\n",
       " 1472,\n",
       " 1473,\n",
       " 1474,\n",
       " 1475,\n",
       " 1476,\n",
       " 1477,\n",
       " 1478,\n",
       " 1479,\n",
       " 1480,\n",
       " 1481,\n",
       " 1482,\n",
       " 1483,\n",
       " 1520,\n",
       " 1521,\n",
       " 1522,\n",
       " 1523,\n",
       " 1524,\n",
       " 1525,\n",
       " 1526,\n",
       " 1527,\n",
       " 1528,\n",
       " 1560,\n",
       " 1561,\n",
       " 1600,\n",
       " 1601,\n",
       " 1602,\n",
       " 1603,\n",
       " 1604,\n",
       " 1605,\n",
       " 1606,\n",
       " 1607,\n",
       " 1608,\n",
       " 1609,\n",
       " 1610,\n",
       " 1611,\n",
       " 1612,\n",
       " 1613,\n",
       " 1640,\n",
       " 1641,\n",
       " 1642,\n",
       " 1643,\n",
       " 1644,\n",
       " 1645,\n",
       " 1646,\n",
       " 1647,\n",
       " 1648,\n",
       " 1649,\n",
       " 1650,\n",
       " 1651,\n",
       " 1652,\n",
       " 1680,\n",
       " 1681,\n",
       " 1720,\n",
       " 1721,\n",
       " 1760,\n",
       " 1761,\n",
       " 1800,\n",
       " 1801,\n",
       " 1802,\n",
       " 1803,\n",
       " 1840,\n",
       " 1841,\n",
       " 1842,\n",
       " 1843,\n",
       " 1844,\n",
       " 1845,\n",
       " 1846,\n",
       " 1847,\n",
       " 1848,\n",
       " 1849,\n",
       " 1850,\n",
       " 1851,\n",
       " 1852,\n",
       " 1853,\n",
       " 1854,\n",
       " 1855,\n",
       " 1856,\n",
       " 1857,\n",
       " 1858,\n",
       " 1880,\n",
       " 1881,\n",
       " 1882,\n",
       " 1883,\n",
       " 1884,\n",
       " 1885,\n",
       " 1886,\n",
       " 1887,\n",
       " 1888,\n",
       " 1889,\n",
       " 1890,\n",
       " 1891,\n",
       " 1892,\n",
       " 1920,\n",
       " 1960,\n",
       " 1961,\n",
       " 1962,\n",
       " 1963,\n",
       " 2000,\n",
       " 2001,\n",
       " 2002,\n",
       " 2003,\n",
       " 2004,\n",
       " 2005,\n",
       " 2006,\n",
       " 2007,\n",
       " 2008,\n",
       " 2009,\n",
       " 2010,\n",
       " 2011,\n",
       " 2012,\n",
       " 2040,\n",
       " 2041,\n",
       " 2080,\n",
       " 2081,\n",
       " 2082,\n",
       " 2083,\n",
       " 2084,\n",
       " 2085,\n",
       " 2086,\n",
       " 2087,\n",
       " 2088,\n",
       " 2089,\n",
       " 2090,\n",
       " 2091,\n",
       " 2120,\n",
       " 2121,\n",
       " 2160,\n",
       " 2161,\n",
       " 2162,\n",
       " 2163,\n",
       " 2164,\n",
       " 2165,\n",
       " 2166,\n",
       " 2200,\n",
       " 2201,\n",
       " 2202,\n",
       " 2203,\n",
       " 2204,\n",
       " 2205,\n",
       " 2206,\n",
       " 2207,\n",
       " 2208,\n",
       " 2209,\n",
       " 2210,\n",
       " 2211,\n",
       " 2212,\n",
       " 2213,\n",
       " 2214,\n",
       " 2215,\n",
       " 2216,\n",
       " 2217,\n",
       " 2218,\n",
       " 2240,\n",
       " 2241,\n",
       " 2280,\n",
       " 2320,\n",
       " 2321,\n",
       " 2322,\n",
       " 2323,\n",
       " 2324,\n",
       " 2360,\n",
       " 2361,\n",
       " 2362,\n",
       " 2400,\n",
       " 2401,\n",
       " 2402,\n",
       " 2403,\n",
       " 2404,\n",
       " 2440,\n",
       " 2441,\n",
       " 2442,\n",
       " 2443,\n",
       " 2480,\n",
       " 2481,\n",
       " 2482,\n",
       " 2483,\n",
       " 2484,\n",
       " 2485,\n",
       " 2520,\n",
       " 2521,\n",
       " 2522]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.view(-1,283)#[\n",
    "[(e[0] * lengths.max() + e[1]).item() for e in real_seq_pos_per_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e96ada49-49ea-43d3-b868-a70262fef6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(0,10,size=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1eabd5d1-dc52-4c16-9294-200407d59d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 2, 1, 1, 7, 7, 9, 1, 2],\n",
       "        [7, 5, 2, 5, 2, 0, 2, 8, 2, 0],\n",
       "        [4, 8, 9, 7, 0, 1, 8, 1, 3, 7],\n",
       "        [6, 8, 0, 2, 1, 4, 7, 4, 6, 3],\n",
       "        [1, 4, 0, 2, 7, 2, 8, 9, 4, 4],\n",
       "        [8, 9, 6, 6, 9, 4, 9, 3, 9, 8],\n",
       "        [8, 2, 1, 2, 6, 6, 1, 6, 8, 9],\n",
       "        [7, 8, 7, 8, 0, 7, 6, 3, 3, 2],\n",
       "        [2, 3, 3, 3, 5, 1, 0, 2, 3, 0],\n",
       "        [6, 2, 6, 6, 1, 1, 2, 7, 0, 7]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c82fe8ac-5782-4278-b34d-ee5a0a80fe70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 5, 2, 5, 2, 0, 2, 8, 2, 0],\n",
       "        [6, 8, 0, 2, 1, 4, 7, 4, 6, 3],\n",
       "        [8, 9, 6, 6, 9, 4, 9, 3, 9, 8]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[1,3,5],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb561b-18c8-486b-8f95-ee9b7191796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "336, 485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37374f62-57d6-4eed-bea2-d0c624b52c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    history_sequences, target_sequences = batch['train_sequences']['sequence'], batch['target_sequences']['sequence']\n",
    "    \n",
    "    outs = model(history_sequences,target_sequences)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4334e054-6fbf-4d7d-8de7-0bf1c7b5d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_1 = 79\n",
    "diag_2 = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0befee19-34ab-4e4e-b1de-8edb56879278",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_1, tpr, ths = f.roc(outs.view(-1,283)[:,diag_1],target_sequences.view(-1,283)[:,diag_1])\n",
    "fpr_2, tpr, ths = f.roc(outs.view(-1,283)[:,diag_2],target_sequences.view(-1,283)[:,diag_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "efbc0475-adb6-4259-a95b-c194b98ee3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(25.)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sequences.view(-1,283)[:,diag_1].sum()\n",
    "target_sequences.view(-1,283)[:,diag_2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fb273dc7-d831-4d6e-82cc-ce2c7981ac2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.1155, -3.4331, -3.0590,  ..., -0.1427, -0.1427, -0.1427],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-3.9595, -3.6251, -3.6365,  ..., -0.2735, -0.2735, -0.2735],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.view(-1,283)[:,diag_1]\n",
    "outs.view(-1,283)[:,diag_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1af08ea8-a573-499d-b0d1-da8d160d0b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.8435, 0.8438, 0.8442, 0.8445])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "508448d5-4691-4ffd-86a6-4d7d22a2075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.8438, 0.8444, 0.8447, 0.8451])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "043403b1-7ed1-40c2-b655-b9a74ce686f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([494])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([494])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_1.shape\n",
    "fpr_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "53e9c9ab-c14b-4016-a5a4-52a08e28588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = f.roc(outs.view(-1,283),target_sequences.view(-1,283),num_classes=283)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2d264a5b-b14b-4853-b751-6732c5d05a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([494])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29e657fe-bf5a-4fe4-a009-251d97cbb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import functional as f\n",
    "\n",
    "def compute_metricsV3(model, dataloader):\n",
    "    \n",
    "    \n",
    "    recall = list()\n",
    "    precision = list()\n",
    "    f1 = list()\n",
    "    for i,batch in tqdm(enumerate(iter(dataloader))):\n",
    "        history_sequences, target_sequences = batch['train_sequences']['sequence'],batch['target_sequences']['sequence']\n",
    "        outs = model(history_sequences,target_sequences)\n",
    "        \n",
    "        recall.append(f.recall(outs.view(-1,283),target_sequences.int().view(-1,283),top_k=30,average='samples') * target_sequences.shape[0] * target_sequences.shape[1] / target_sequences.any(dim=-1).sum())\n",
    "        precision.append(f.precision(outs.view(-1,283),target_sequences.int().view(-1,283),top_k=30,average='samples') * target_sequences.shape[0] * target_sequences.shape[1] / target_sequences.any(dim=-1).sum())\n",
    "        f1.append(f.f1_score(outs.view(-1,283),target_sequences.int().view(-1,283),top_k=30,average='samples') * target_sequences.shape[0] * target_sequences.shape[1] / target_sequences.any(dim=-1).sum())\n",
    "    \n",
    "    return {'recall@30':np.mean(recall),\n",
    "            'precision@30':np.mean(precision),\n",
    "            'f1@30':np.mean(f1)\n",
    "           }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8359dc-2fe6-4429-90d2-6f792a5a827d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(model_outputs,model_predictions,golden,metrics,mode='adm'):\n",
    "    \"\"\"\n",
    "    all input dataframes must be of the form:\n",
    "    double index of (<pat_id>,>adm_index>)\n",
    "    and columns are the diagnostics. eg: diag_0,...,diag_272\n",
    "    \n",
    "    returns several metrics in a dataframe\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    metrics : list\n",
    "        ['roc,avgprec','acc','recall','precision','f1']\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    \n",
    "    accepted = ['roc','avgprec','acc','recall','accuracy','precision','f1','recall@','precision@','f1@']\n",
    "    \n",
    "    diag_weights = golden.sum(axis=0)\n",
    "    adm_weights = golden.sum(axis=1)\n",
    "    \n",
    "    if metrics == 'all':\n",
    "        metrics = accepted\n",
    "    \n",
    "    assert len(metrics) > 0\n",
    "    assert any([e in metrics for e in accepted]) or any([e for e in metrics if 'recall@' in e])\n",
    "    \n",
    "    # threshold independent\n",
    "    diag_metrics = list()\n",
    "    adm_metrics = list()\n",
    "    res_metrics = list()\n",
    "    \n",
    "    if 'roc' in metrics:\n",
    "        print('computing roc')\n",
    "        roc = model_outputs.progress_apply(lambda row: roc_auc_score(golden.loc[row.name],row) if any(golden.loc[row.name] == 1) else np.nan,axis=1).rename('roc_adm') if mode=='adm' else model_outputs.progress_apply(lambda col: roc_auc_score(golden[col.name],col) if any(golden[col.name] == 1) else np.nan).rename('roc_diag')\n",
    "        #roc_diag = model_outputs.apply(lambda col: roc_auc_score(golden[col.name],col) if any(golden[col.name] == 1) else np.nan).rename('roc_diag')\n",
    "        #roc_adm = model_outputs.apply(lambda row: roc_auc_score(golden.loc[row.name],row) if any(golden.loc[row.name] == 1) else np.nan,axis=1).rename('roc_adm')\n",
    "        #diag_metrics.append(roc_diag)\n",
    "        #adm_metrics.append(roc_adm)\n",
    "        res_metrics.append(roc)\n",
    "    \n",
    "    if 'avgprec' in metrics:\n",
    "        avgprec_diag = model_outputs.apply(lambda col: average_precision_score(golden[col.name],col) if any(golden[col.name] == 1) else np.nan).rename('avgprec_diag')\n",
    "        avgprec_adm = model_outputs.apply(lambda row: average_precision_score(golden.loc[row.name],row) if any(golden.loc[row.name] == 1) else np.nan,axis=1).rename('avgprec_adm')\n",
    "        diag_metrics.append(avgprec_diag)\n",
    "        adm_metrics.append(avgprec_adm)\n",
    "\n",
    "    # threshold dependent\n",
    "    \n",
    "    if 'accuracy' in metrics:\n",
    "        accuracy_diag = model_predictions.apply(lambda col: accuracy_score(golden[col.name],col) if any(golden[col.name] == 1) else np.nan).rename('accuracy_diag')\n",
    "        accuracy_adm = model_predictions.apply(lambda row: accuracy_score(golden.loc[row.name],row) if any(golden.loc[row.name] == 1) else np.nan,axis=1).rename('accuracy_adm')\n",
    "        diag_metrics.append(accuracy_diag)\n",
    "        adm_metrics.append(accuracy_adm)\n",
    "\n",
    "    if 'recall' in metrics:\n",
    "        recall_diag = model_predictions.apply(lambda col: recall_score(golden[col.name],col,zero_division=0)).rename('recall_diag')\n",
    "        recall_adm = model_predictions.apply(lambda row: recall_score(golden.loc[row.name],row,zero_division=0),axis=1).rename('recall_adm')\n",
    "        diag_metrics.append(recall_diag)\n",
    "        adm_metrics.append(recall_adm)\n",
    "\n",
    "    if 'precision' in metrics:\n",
    "        precision_diag = model_predictions.apply(lambda col: precision_score(golden[col.name],col) if any(model_predictions[col.name] == 1) else np.nan).rename('precision_diag')\n",
    "        precision_adm = model_predictions.apply(lambda row: precision_score(golden.loc[row.name],row) if any(model_predictions.loc[row.name] == 1) else np.nan,axis=1).rename('precision_adm')\n",
    "        diag_metrics.append(precision_diag)\n",
    "        adm_metrics.append(precision_adm)\n",
    "\n",
    "    if 'f1' in metrics:\n",
    "        f1_diag = model_predictions.apply(lambda col: f1_score(golden[col.name],col) if any(golden[col.name] == 1) else np.nan).rename('f1_diag')\n",
    "        f1_adm = model_predictions.apply(lambda row: f1_score(golden.loc[row.name],row) if any(golden.loc[row.name] == 1) else np.nan,axis=1).rename('f1_adm')\n",
    "        diag_metrics.append(f1_diag)\n",
    "        adm_metrics.append(f1_adm)\n",
    "    \n",
    "    # i.e. if recall@k in metrics\n",
    "    if any(filter(lambda x: re.match('\\w+@\\d+',x), metrics)):\n",
    "        \n",
    "        matches = [e[0] for e in [re.findall('\\w+@\\d+',e) for e in metrics] if e] # get all <metric>@k in metrics (there may be multiple)\n",
    "        for match in matches:\n",
    "            \n",
    "            k = int(re.findall('\\w+@(\\d+)',match)[0])\n",
    "            metric = re.findall('(\\w+)@\\d+',match)[0]\n",
    "            \n",
    "            topk_outputs = model_outputs.apply(lambda row: row.nlargest(k),axis=1)\n",
    "\n",
    "            # fix missing columns from previous operation\n",
    "            missing_cols = [col for col in model_outputs.columns if col not in topk_outputs.columns]\n",
    "            topk_outputs_all_cols = pd.concat([topk_outputs,pd.DataFrame(columns=missing_cols)])\n",
    "            topk_outputs_all_cols = topk_outputs_all_cols[model_outputs.columns]\n",
    "            \n",
    "            ## sometimes k > (#logits>0) so we will turn all 0 logits into nan so that the following lines don't convert them to predictions\n",
    "            topk_outputs_all_cols = topk_outputs_all_cols.mask(topk_outputs_all_cols == 0,np.nan)\n",
    "            # done, continuing...\n",
    "\n",
    "            topk_predictions = np.where(topk_outputs_all_cols.isna(),0,1)\n",
    "            topk_predictions = pd.DataFrame(data=topk_predictions,columns=model_outputs.columns,index=model_outputs.index)\n",
    "\n",
    "            if metric == 'recall':\n",
    "                print(f'computing recall@{k}')\n",
    "                metric_at_k = (topk_predictions\n",
    "                               .progress_apply(lambda row: recall_score(golden.loc[row.name],row,zero_division=0),axis=1)\n",
    "                               .rename(f'recall@{k}_adm') \n",
    "                               if mode=='adm' else \n",
    "                               topk_predictions\n",
    "                               .progress_apply(lambda col: recall_score(golden[col.name],col,zero_division=0))\n",
    "                               .rename(f'recall@{k}_diag')\n",
    "                              )\n",
    "                #metric_at_k_diag = topk_predictions.apply(lambda col: recall_score(golden[col.name],col,zero_division=0)).rename(f'recall@{k}_diag')\n",
    "                #metric_at_k_adm = topk_predictions.apply(lambda row: recall_score(golden.loc[row.name],row,zero_division=0),axis=1).rename(f'recall@{k}_adm')\n",
    "            \n",
    "            elif metric == 'precision':\n",
    "                print(f'computing precision@{k}')\n",
    "                metric_at_k = (topk_predictions\n",
    "                                .progress_apply(lambda row: precision_score(golden.loc[row.name],row) \n",
    "                                       if any(topk_predictions.loc[row.name] == 1) else np.nan,axis=1)\n",
    "                                .rename(f'precision@{k}_adm') \n",
    "                                if mode=='adm' else \n",
    "                                topk_predictions\n",
    "                                .progress_apply(lambda col: precision_score(golden[col.name],col) \n",
    "                                       if any(topk_predictions[col.name] == 1) else np.nan)\n",
    "                                .rename(f'precision@{k}_diag')\n",
    "                               )\n",
    "                #metric_at_k_diag = topk_predictions.apply(lambda col: precision_score(golden[col.name],col) if any(topk_predictions[col.name] == 1) else np.nan).rename(f'precision@{k}_diag')\n",
    "                #metric_at_k_adm = topk_predictions.apply(lambda row: precision_score(golden.loc[row.name],row) if any(topk_predictions.loc[row.name] == 1) else np.nan,axis=1).rename(f'precision@{k}_adm')\n",
    "                \n",
    "            elif metric == 'f1':\n",
    "                metric_at_k_diag = topk_predictions.apply(lambda col: f1_score(golden[col.name],col) if any(golden[col.name] == 1) else np.nan).rename(f'f1@{k}_diag')\n",
    "                metric_at_k_adm = topk_predictions.apply(lambda row: f1_score(golden.loc[row.name],row) if any(golden.loc[row.name] == 1) else np.nan,axis=1).rename(f'f1@{k}_adm')\n",
    "            \n",
    "            else:\n",
    "                print('what is happening')\n",
    "                print(metric)\n",
    "\n",
    "            #diag_metrics.append(metric_at_k_diag)\n",
    "            #adm_metrics.append(metric_at_k_adm)    \n",
    "            res_metrics.append(metric_at_k)\n",
    "    \n",
    "    # take weighted average\n",
    "    \"\"\"\n",
    "    diag_metrics_wavg = (pd.concat(diag_metrics,axis=1)\n",
    "                         .multiply(diag_weights,axis=0)\n",
    "                         .sum(axis=0)\n",
    "                         .divide(\n",
    "                             diag_weights.sum()\n",
    "                         )\n",
    "                        )\n",
    "    \n",
    "    adm_metrics_wavg = (pd.concat(adm_metrics,axis=1)\n",
    "                        .multiply(adm_weights,axis=0)\n",
    "                        .sum(axis=0)\n",
    "                        .divide(\n",
    "                            adm_weights.sum()\n",
    "                        )\n",
    "                       )\n",
    "    \"\"\"\n",
    "    #diag_metrics_wavg = (pd.concat(diag_metrics,axis=1)\n",
    "    #                     .mean(axis=0)\n",
    "    #                    )\n",
    "    \n",
    "    #adm_metrics_wavg = (pd.concat(adm_metrics,axis=1)\n",
    "    #                     .mean(axis=0)\n",
    "    #                    )\n",
    "\n",
    "    #res = pd.concat([diag_metrics_wavg,adm_metrics_wavg])\n",
    "    res = pd.concat(res_metrics,axis=1).mean(axis=0)\n",
    "    res.index.name = 'metrics'\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fa062-c0e8-4c42-a02c-b292fce094ed",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e130cc9-e824-465e-be62-640b1feee592",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'tmp_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddd2c523-38ec-40a8-8b94-e9cd22201a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e7c164a9db4b98a347af064bfd27ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:imta4mp6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.139 MB of 0.139 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">drawn-flower-40</strong>: <a href=\"https://wandb.ai/snovaisg/icare/runs/imta4mp6\" target=\"_blank\">https://wandb.ai/snovaisg/icare/runs/imta4mp6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220726_124256-imta4mp6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:imta4mp6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/debian/Simao/master-thesis/wandb/run-20220726_124309-2c1crvsf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/snovaisg/icare/runs/2c1crvsf\" target=\"_blank\">hopeful-wood-41</a></strong> to <a href=\"https://wandb.ai/snovaisg/icare\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/snovaisg/icare/runs/2c1crvsf?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7f58241ae0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f055553cf4dd4e619acacccd06e41dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bd24ae203d404b850f9d28d8c65c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train each batch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b4f4102b6f44bea6d43b79f06413c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_metricsV3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_one_epochV2(model,train_dataloader,epoch,criterion,optimizer);\n\u001b[1;32m     30\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:epoch,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m:loss})\n\u001b[0;32m---> 34\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metricsV3\u001b[49m(model,train_dataloader)\n\u001b[1;32m     35\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:train_metrics[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m train_metrics}\n\u001b[1;32m     37\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m compute_metricsV3(model,val_dataloader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_metricsV3' is not defined"
     ]
    }
   ],
   "source": [
    "#param_set = {\n",
    "#          'hidden_size':100,\n",
    "#          'num_layers':1,\n",
    "##          'lr':0.01,\n",
    "#          'model':'rnn'\n",
    "#         }\n",
    "for idx,param_set in tqdm(enumerate(params)):\n",
    "    config = {**param_set, \n",
    "              **meta_parameters}\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"icare\", \n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    model = RNN(input_size=input_size,\n",
    "              hidden_size=config['hidden_size'],\n",
    "              num_layers=config['num_layers'],\n",
    "              n_labels=n_labels,\n",
    "              model=config['model'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    loss = compute_loss(model,train_dataloader)\n",
    "    wandb.log({'epoch':0,'loss':loss})\n",
    "    \n",
    "    print('Training each epoch')\n",
    "    for epoch in tqdm(range(1,config['epochs']+1)):\n",
    "        \n",
    "        loss = train_one_epochV2(model,train_dataloader,epoch,criterion,optimizer);\n",
    "        wandb.log({'epoch':epoch,'loss':loss})\n",
    "        \n",
    "        \n",
    "    \n",
    "    train_metrics = compute_metricsV3(model,train_dataloader)\n",
    "    train_metrics = {f'train_{k}':train_metrics[k] for k in train_metrics}\n",
    "    \n",
    "    val_metrics = compute_metricsV3(model,val_dataloader)\n",
    "    val_metrics = {f'val_{k}':val_metrics[k] for k in val_metrics}    \n",
    "\n",
    "    log = dict()\n",
    "\n",
    "    log.update(train_metrics)\n",
    "    log.update(val_metrics)\n",
    "\n",
    "    wandb.log(log)\n",
    "    \n",
    "    model_name = str(param_set)\n",
    "\n",
    "    hypp_save_path = os.path.join(model_folder, model_name+'_hyper_parameters.json')\n",
    "\n",
    "    with open(hypp_save_path, \"w\") as file:\n",
    "        json.dump(params, file)\n",
    "\n",
    "    print('Hyperparameters saved!')\n",
    "    \n",
    "    weights_save_path = os.path.join(model_folder,model_name+\"_weights\")\n",
    "\n",
    "    torch.save(model.state_dict(), \n",
    "               weights_save_path\n",
    "              )\n",
    "    print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b382fd-66bd-48ec-a8f2-102203430198",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c08cb4c5-5ab9-4799-afa2-334ec2593a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a174b-1018-4307-9457-56eeac2ce6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87591116-96eb-4551-ad82-0f22f471a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "df = pd.DataFrame(np.zeros(shape=(int(1e8),10)))\n",
    "#df_later = df.copy()\n",
    "#df_later.loc[int(1e7)] = 1\n",
    "#df_later = df_later.astype(bool)\n",
    "#\n",
    "#df_sooner = df.copy()\n",
    "#df_sooner.loc[int(1e2)] = 1\n",
    "#df_sooner = df_sooner.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4133f721-89e6-462d-963f-37d783771b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.47 s ± 89.3 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10 -n 1\n",
    "\n",
    "df_sooner = df.copy()\n",
    "df_sooner.loc[int(1e2)] = 1\n",
    "df_sooner = df_sooner.astype(bool)\n",
    "df_sooner.iloc[:,0].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78936b2c-1350-484e-a8e2-a5ce4bc124f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.44 s ± 42.9 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10 -n 1\n",
    "\n",
    "df_later = df.copy()\n",
    "df_later.loc[int(1e7)] = 1\n",
    "df_later = df_later.astype(bool)\n",
    "df_later.iloc[:,0].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a787957b-9971-4ec2-b9b6-3c6a1f38fae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            0.0\n",
       "1            0.0\n",
       "2            0.0\n",
       "3            0.0\n",
       "4            0.0\n",
       "            ... \n",
       "999999995    0.0\n",
       "999999996    0.0\n",
       "999999997    0.0\n",
       "999999998    0.0\n",
       "999999999    0.0\n",
       "Length: 1000000000, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.mask(array.index == int(1e2)).fillna(1)#.loc[int(1e2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a996747-eeb6-4fb1-af5c-0815a10ac722",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1981a5ea-bce5-4312-b9e5-0f6d316df48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_roc_optimized(logits,golden):\n",
    "    idx = pd.IndexSlice\n",
    "    \n",
    "    logits_ = logits.copy()\n",
    "    golden_ = golden.copy()\n",
    "    \n",
    "    logits_.columns = pd.MultiIndex.from_product([['logits'],logits_.columns])\n",
    "    golden_.columns = pd.MultiIndex.from_product([['golden'],golden_.columns])\n",
    "    \n",
    "    full = logits_.join(golden_,how='inner')\n",
    "    assert (full.shape[0] == logits.shape[0]) and (full.shape[0] == golden.shape[0]),'oops'\n",
    "    \n",
    "    return full.apply(lambda row: roc_auc_score(row.loc[:,idx['golden',:]],row.loc[:,idx['logits']]) if any(\n",
    "    #return logits.apply(lambda row: roc_auc_score(golden.loc[row.name],row) if any(golden.loc[row.name] == 1) else np.nan,axis=1).rename('roc_adm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df78d02d-ecf4-4a1e-b6af-4e89a1f8fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270248\n",
      "270248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">logits</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">golden</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>diag_0</th>\n",
       "      <th>diag_1</th>\n",
       "      <th>diag_2</th>\n",
       "      <th>diag_3</th>\n",
       "      <th>diag_4</th>\n",
       "      <th>diag_5</th>\n",
       "      <th>diag_6</th>\n",
       "      <th>diag_7</th>\n",
       "      <th>diag_8</th>\n",
       "      <th>diag_9</th>\n",
       "      <th>...</th>\n",
       "      <th>diag_273</th>\n",
       "      <th>diag_274</th>\n",
       "      <th>diag_275</th>\n",
       "      <th>diag_276</th>\n",
       "      <th>diag_277</th>\n",
       "      <th>diag_278</th>\n",
       "      <th>diag_279</th>\n",
       "      <th>diag_280</th>\n",
       "      <th>diag_281</th>\n",
       "      <th>diag_282</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pat_id</th>\n",
       "      <th>adm_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">00070121385D5F499BB0D98F48554EF5</th>\n",
       "      <th>1</th>\n",
       "      <td>0.521170</td>\n",
       "      <td>0.487607</td>\n",
       "      <td>0.482274</td>\n",
       "      <td>0.475589</td>\n",
       "      <td>0.552587</td>\n",
       "      <td>0.507825</td>\n",
       "      <td>0.506065</td>\n",
       "      <td>0.470086</td>\n",
       "      <td>0.530118</td>\n",
       "      <td>0.447996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509584</td>\n",
       "      <td>0.494575</td>\n",
       "      <td>0.494839</td>\n",
       "      <td>0.497005</td>\n",
       "      <td>0.532272</td>\n",
       "      <td>0.519580</td>\n",
       "      <td>0.491184</td>\n",
       "      <td>0.466202</td>\n",
       "      <td>0.519124</td>\n",
       "      <td>0.462451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.513210</td>\n",
       "      <td>0.491401</td>\n",
       "      <td>0.484080</td>\n",
       "      <td>0.488004</td>\n",
       "      <td>0.558111</td>\n",
       "      <td>0.507040</td>\n",
       "      <td>0.501613</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.531054</td>\n",
       "      <td>0.443273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0008F5D602267E1E2F679BA745F38A41</th>\n",
       "      <th>1</th>\n",
       "      <td>0.517556</td>\n",
       "      <td>0.494975</td>\n",
       "      <td>0.507667</td>\n",
       "      <td>0.483336</td>\n",
       "      <td>0.539981</td>\n",
       "      <td>0.499710</td>\n",
       "      <td>0.504512</td>\n",
       "      <td>0.480275</td>\n",
       "      <td>0.520952</td>\n",
       "      <td>0.457920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.515604</td>\n",
       "      <td>0.504581</td>\n",
       "      <td>0.508042</td>\n",
       "      <td>0.504277</td>\n",
       "      <td>0.531513</td>\n",
       "      <td>0.508115</td>\n",
       "      <td>0.496231</td>\n",
       "      <td>0.483177</td>\n",
       "      <td>0.515269</td>\n",
       "      <td>0.446479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFFEF32F9705DCBB22EE8E7CC09E9379</th>\n",
       "      <th>2</th>\n",
       "      <td>0.514425</td>\n",
       "      <td>0.498155</td>\n",
       "      <td>0.501106</td>\n",
       "      <td>0.511381</td>\n",
       "      <td>0.542690</td>\n",
       "      <td>0.518909</td>\n",
       "      <td>0.527698</td>\n",
       "      <td>0.480515</td>\n",
       "      <td>0.523945</td>\n",
       "      <td>0.445996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">FFFFCAF4C44303A609ABA747E6E00CEE</th>\n",
       "      <th>1</th>\n",
       "      <td>0.515025</td>\n",
       "      <td>0.496486</td>\n",
       "      <td>0.507078</td>\n",
       "      <td>0.495090</td>\n",
       "      <td>0.541965</td>\n",
       "      <td>0.511540</td>\n",
       "      <td>0.508252</td>\n",
       "      <td>0.473659</td>\n",
       "      <td>0.524212</td>\n",
       "      <td>0.460159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511931</td>\n",
       "      <td>0.491205</td>\n",
       "      <td>0.494301</td>\n",
       "      <td>0.516626</td>\n",
       "      <td>0.528295</td>\n",
       "      <td>0.506213</td>\n",
       "      <td>0.500183</td>\n",
       "      <td>0.484189</td>\n",
       "      <td>0.529203</td>\n",
       "      <td>0.454736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">FFFFCD298CB71236CEB3470483A4C6A1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.509109</td>\n",
       "      <td>0.493436</td>\n",
       "      <td>0.502984</td>\n",
       "      <td>0.504799</td>\n",
       "      <td>0.543310</td>\n",
       "      <td>0.513838</td>\n",
       "      <td>0.511638</td>\n",
       "      <td>0.478368</td>\n",
       "      <td>0.537129</td>\n",
       "      <td>0.461304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.502257</td>\n",
       "      <td>0.496711</td>\n",
       "      <td>0.490319</td>\n",
       "      <td>0.521365</td>\n",
       "      <td>0.549081</td>\n",
       "      <td>0.507638</td>\n",
       "      <td>0.489667</td>\n",
       "      <td>0.482179</td>\n",
       "      <td>0.523335</td>\n",
       "      <td>0.434089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270248 rows × 566 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              logits                      \\\n",
       "                                              diag_0    diag_1    diag_2   \n",
       "pat_id                           adm_index                                 \n",
       "00070121385D5F499BB0D98F48554EF5 1          0.521170  0.487607  0.482274   \n",
       "                                 2          0.509584  0.494575  0.494839   \n",
       "                                 3          0.513210  0.491401  0.484080   \n",
       "0008F5D602267E1E2F679BA745F38A41 1          0.517556  0.494975  0.507667   \n",
       "                                 2          0.515604  0.504581  0.508042   \n",
       "...                                              ...       ...       ...   \n",
       "FFFEF32F9705DCBB22EE8E7CC09E9379 2          0.514425  0.498155  0.501106   \n",
       "FFFFCAF4C44303A609ABA747E6E00CEE 1          0.515025  0.496486  0.507078   \n",
       "                                 2          0.511931  0.491205  0.494301   \n",
       "FFFFCD298CB71236CEB3470483A4C6A1 1          0.509109  0.493436  0.502984   \n",
       "                                 2          0.502257  0.496711  0.490319   \n",
       "\n",
       "                                                                          \\\n",
       "                                              diag_3    diag_4    diag_5   \n",
       "pat_id                           adm_index                                 \n",
       "00070121385D5F499BB0D98F48554EF5 1          0.475589  0.552587  0.507825   \n",
       "                                 2          0.497005  0.532272  0.519580   \n",
       "                                 3          0.488004  0.558111  0.507040   \n",
       "0008F5D602267E1E2F679BA745F38A41 1          0.483336  0.539981  0.499710   \n",
       "                                 2          0.504277  0.531513  0.508115   \n",
       "...                                              ...       ...       ...   \n",
       "FFFEF32F9705DCBB22EE8E7CC09E9379 2          0.511381  0.542690  0.518909   \n",
       "FFFFCAF4C44303A609ABA747E6E00CEE 1          0.495090  0.541965  0.511540   \n",
       "                                 2          0.516626  0.528295  0.506213   \n",
       "FFFFCD298CB71236CEB3470483A4C6A1 1          0.504799  0.543310  0.513838   \n",
       "                                 2          0.521365  0.549081  0.507638   \n",
       "\n",
       "                                                                          \\\n",
       "                                              diag_6    diag_7    diag_8   \n",
       "pat_id                           adm_index                                 \n",
       "00070121385D5F499BB0D98F48554EF5 1          0.506065  0.470086  0.530118   \n",
       "                                 2          0.491184  0.466202  0.519124   \n",
       "                                 3          0.501613  0.471400  0.531054   \n",
       "0008F5D602267E1E2F679BA745F38A41 1          0.504512  0.480275  0.520952   \n",
       "                                 2          0.496231  0.483177  0.515269   \n",
       "...                                              ...       ...       ...   \n",
       "FFFEF32F9705DCBB22EE8E7CC09E9379 2          0.527698  0.480515  0.523945   \n",
       "FFFFCAF4C44303A609ABA747E6E00CEE 1          0.508252  0.473659  0.524212   \n",
       "                                 2          0.500183  0.484189  0.529203   \n",
       "FFFFCD298CB71236CEB3470483A4C6A1 1          0.511638  0.478368  0.537129   \n",
       "                                 2          0.489667  0.482179  0.523335   \n",
       "\n",
       "                                                      ...   golden           \\\n",
       "                                              diag_9  ... diag_273 diag_274   \n",
       "pat_id                           adm_index            ...                     \n",
       "00070121385D5F499BB0D98F48554EF5 1          0.447996  ...      0.0      0.0   \n",
       "                                 2          0.462451  ...      0.0      0.0   \n",
       "                                 3          0.443273  ...      0.0      0.0   \n",
       "0008F5D602267E1E2F679BA745F38A41 1          0.457920  ...      0.0      0.0   \n",
       "                                 2          0.446479  ...      0.0      0.0   \n",
       "...                                              ...  ...      ...      ...   \n",
       "FFFEF32F9705DCBB22EE8E7CC09E9379 2          0.445996  ...      0.0      0.0   \n",
       "FFFFCAF4C44303A609ABA747E6E00CEE 1          0.460159  ...      0.0      0.0   \n",
       "                                 2          0.454736  ...      0.0      0.0   \n",
       "FFFFCD298CB71236CEB3470483A4C6A1 1          0.461304  ...      0.0      0.0   \n",
       "                                 2          0.434089  ...      0.0      0.0   \n",
       "\n",
       "                                                                       \\\n",
       "                                           diag_275 diag_276 diag_277   \n",
       "pat_id                           adm_index                              \n",
       "00070121385D5F499BB0D98F48554EF5 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "                                 3              0.0      0.0      0.0   \n",
       "0008F5D602267E1E2F679BA745F38A41 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "...                                             ...      ...      ...   \n",
       "FFFEF32F9705DCBB22EE8E7CC09E9379 2              0.0      0.0      0.0   \n",
       "FFFFCAF4C44303A609ABA747E6E00CEE 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "FFFFCD298CB71236CEB3470483A4C6A1 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "\n",
       "                                                                       \\\n",
       "                                           diag_278 diag_279 diag_280   \n",
       "pat_id                           adm_index                              \n",
       "00070121385D5F499BB0D98F48554EF5 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "                                 3              0.0      0.0      0.0   \n",
       "0008F5D602267E1E2F679BA745F38A41 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "...                                             ...      ...      ...   \n",
       "FFFEF32F9705DCBB22EE8E7CC09E9379 2              0.0      0.0      0.0   \n",
       "FFFFCAF4C44303A609ABA747E6E00CEE 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "FFFFCD298CB71236CEB3470483A4C6A1 1              0.0      0.0      0.0   \n",
       "                                 2              0.0      0.0      0.0   \n",
       "\n",
       "                                                              \n",
       "                                           diag_281 diag_282  \n",
       "pat_id                           adm_index                    \n",
       "00070121385D5F499BB0D98F48554EF5 1              0.0      0.0  \n",
       "                                 2              0.0      0.0  \n",
       "                                 3              0.0      0.0  \n",
       "0008F5D602267E1E2F679BA745F38A41 1              0.0      0.0  \n",
       "                                 2              0.0      0.0  \n",
       "...                                             ...      ...  \n",
       "FFFEF32F9705DCBB22EE8E7CC09E9379 2              0.0      0.0  \n",
       "FFFFCAF4C44303A609ABA747E6E00CEE 1              0.0      0.0  \n",
       "                                 2              0.0      0.0  \n",
       "FFFFCD298CB71236CEB3470483A4C6A1 1              0.0      0.0  \n",
       "                                 2              0.0      0.0  \n",
       "\n",
       "[270248 rows x 566 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_roc(outs_train,golden_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a87f2d-c1cd-45bb-85d3-44ae6c3242c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m      2\u001b[0m     target \u001b[38;5;241m=\u001b[39m e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(pd\u001b[38;5;241m.\u001b[39mSeries(t)\u001b[38;5;241m.\u001b[39mvalue_counts() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target]):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound one\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m      2\u001b[0m     target \u001b[38;5;241m=\u001b[39m e[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target]):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound one\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Simao/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/base.py:970\u001b[0m, in \u001b[0;36mIndexOpsMixin.value_counts\u001b[0;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_counts\u001b[39m(\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    886\u001b[0m     normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    890\u001b[0m     dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    891\u001b[0m ):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m    Return a Series containing counts of unique values.\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Simao/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/algorithms.py:871\u001b[0m, in \u001b[0;36mvalue_counts\u001b[0;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m         keys, counts \u001b[38;5;241m=\u001b[39m value_counts_arraylike(values, dropna)\n\u001b[0;32m--> 871\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort:\n\u001b[1;32m    874\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39mascending)\n",
      "File \u001b[0;32m~/Simao/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/series.py:380\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# uncomment the line below when removing the FutureWarning\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# dtype = np.dtype(object)\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Simao/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexes/base.py:7043\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7041\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index\u001b[38;5;241m.\u001b[39m_with_infer(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Simao/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexes/base.py:680\u001b[0m, in \u001b[0;36mIndex._with_infer\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    679\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*the Index constructor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 680\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_is_multi:\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"maybe_convert_objects\" has incompatible type\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\u001b[39;00m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     values \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result\u001b[38;5;241m.\u001b[39m_values)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Simao/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexes/base.py:503\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(arr, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype_to_subclass(arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 503\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m disallow_kwargs(kwargs)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass\u001b[38;5;241m.\u001b[39m_simple_new(arr, name)\n",
      "File \u001b[0;32m~/Simao/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexes/numeric.py:165\u001b[0m, in \u001b[0;36mNumericIndex._ensure_array\u001b[0;34m(cls, data, dtype, copy)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_string_data_error(data)\n\u001b[0;32m--> 165\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(data\u001b[38;5;241m.\u001b[39mdtype, dtype):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# TODO: the try/except below is because it's difficult to predict the error\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# and/or error message from different combinations of data and dtype.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Efforts to avoid this try/except welcome.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# See https://github.com/pandas-dev/pandas/pull/41153#discussion_r676206222\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in tqdm(dataset):\n",
    "    target = e['target']\n",
    "    if any([(pd.Series(t).value_counts() >1).any() if t else False for t in target]):\n",
    "        print('found one')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fd3d2-0421-4225-8759-d59d0fda338f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [258.0, 136.0, 258.0],\n",
       " [136.0, 258.0, 47.0, 44.0, 47.0],\n",
       " [258.0, 47.0, 44.0, 47.0],\n",
       " [47.0, 44.0, 47.0],\n",
       " [47.0, 258.0],\n",
       " [258.0],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23211a6-8145-49cf-a7de-7952aefb453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e55a2f57-e207-4139-b9f0-94861092718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(0, int(1e8), (10000, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5db9163-492a-4ae2-bef7-3a3bd2274ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b962258b4148558eb754a247643a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8861896394421316</td>\n",
       "      <td>2516512275852201</td>\n",
       "      <td>372733605816100</td>\n",
       "      <td>4172308873801744</td>\n",
       "      <td>5427823796622756</td>\n",
       "      <td>762071635866025</td>\n",
       "      <td>292111749490729</td>\n",
       "      <td>9997616142086400</td>\n",
       "      <td>6321334108485124</td>\n",
       "      <td>1271447821372689</td>\n",
       "      <td>...</td>\n",
       "      <td>251821352454400</td>\n",
       "      <td>2339752576838121</td>\n",
       "      <td>3704089009201</td>\n",
       "      <td>3943910838718096</td>\n",
       "      <td>6966029135233081</td>\n",
       "      <td>1289798731957824</td>\n",
       "      <td>1984177646603881</td>\n",
       "      <td>300951681465681</td>\n",
       "      <td>7736057397619600</td>\n",
       "      <td>5319091648002609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9155879298676881</td>\n",
       "      <td>6709341313903761</td>\n",
       "      <td>65115732802624</td>\n",
       "      <td>890036469243441</td>\n",
       "      <td>9545029142782225</td>\n",
       "      <td>747917698471056</td>\n",
       "      <td>1806302955388129</td>\n",
       "      <td>1476837230529600</td>\n",
       "      <td>3410535705643264</td>\n",
       "      <td>4293490110523441</td>\n",
       "      <td>...</td>\n",
       "      <td>4681045028294569</td>\n",
       "      <td>2865903717266496</td>\n",
       "      <td>1946334394598400</td>\n",
       "      <td>3823240248510736</td>\n",
       "      <td>2004459901728025</td>\n",
       "      <td>7188269002995600</td>\n",
       "      <td>2243874077724304</td>\n",
       "      <td>6803153467275361</td>\n",
       "      <td>4944864005354529</td>\n",
       "      <td>19988598081321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3498268242745600</td>\n",
       "      <td>669675749072836</td>\n",
       "      <td>8038721756248561</td>\n",
       "      <td>1850330143135296</td>\n",
       "      <td>1072158795994896</td>\n",
       "      <td>28563637494016</td>\n",
       "      <td>1021796322871401</td>\n",
       "      <td>5273630994852900</td>\n",
       "      <td>2872036653400996</td>\n",
       "      <td>6139705831875625</td>\n",
       "      <td>...</td>\n",
       "      <td>509823885318400</td>\n",
       "      <td>916343854253584</td>\n",
       "      <td>3164205151607824</td>\n",
       "      <td>5673962587004100</td>\n",
       "      <td>712247208961600</td>\n",
       "      <td>16249839769881</td>\n",
       "      <td>6765721667556049</td>\n",
       "      <td>5029166424891456</td>\n",
       "      <td>30716934105796</td>\n",
       "      <td>9946115385025369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5311420841104</td>\n",
       "      <td>26637211265625</td>\n",
       "      <td>1822132330619364</td>\n",
       "      <td>5192115845970244</td>\n",
       "      <td>5280184349425129</td>\n",
       "      <td>363343527111184</td>\n",
       "      <td>1167735767278201</td>\n",
       "      <td>6401713074618436</td>\n",
       "      <td>1904908371305536</td>\n",
       "      <td>1169945083891600</td>\n",
       "      <td>...</td>\n",
       "      <td>8993614065000889</td>\n",
       "      <td>4729322665345041</td>\n",
       "      <td>4174562196165904</td>\n",
       "      <td>1025072216529001</td>\n",
       "      <td>40104241849681</td>\n",
       "      <td>1644630079213521</td>\n",
       "      <td>4089657497184900</td>\n",
       "      <td>2461892336851600</td>\n",
       "      <td>319438981596736</td>\n",
       "      <td>188127202106809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7723203564934081</td>\n",
       "      <td>666917970810025</td>\n",
       "      <td>548547784683409</td>\n",
       "      <td>6585170425777969</td>\n",
       "      <td>147144356836</td>\n",
       "      <td>1168733602068516</td>\n",
       "      <td>7887804921289225</td>\n",
       "      <td>2172900609241924</td>\n",
       "      <td>2184455880614569</td>\n",
       "      <td>9670344739843921</td>\n",
       "      <td>...</td>\n",
       "      <td>106726420399104</td>\n",
       "      <td>1490704223710321</td>\n",
       "      <td>2282394441945744</td>\n",
       "      <td>3051335410343236</td>\n",
       "      <td>1148400881668096</td>\n",
       "      <td>5580157529004304</td>\n",
       "      <td>496791675302976</td>\n",
       "      <td>853373183524</td>\n",
       "      <td>5421407382862849</td>\n",
       "      <td>7177021597028416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5766018068358601</td>\n",
       "      <td>99489433364721</td>\n",
       "      <td>1637230981469521</td>\n",
       "      <td>4248013433289796</td>\n",
       "      <td>56538564139369</td>\n",
       "      <td>45277368491716</td>\n",
       "      <td>259256820914116</td>\n",
       "      <td>4111923029021284</td>\n",
       "      <td>9518272623873225</td>\n",
       "      <td>3034494608545009</td>\n",
       "      <td>...</td>\n",
       "      <td>1205378755659225</td>\n",
       "      <td>166931852284521</td>\n",
       "      <td>1725772410201889</td>\n",
       "      <td>7003729359936</td>\n",
       "      <td>166618090411969</td>\n",
       "      <td>7454134896878224</td>\n",
       "      <td>6223394012257081</td>\n",
       "      <td>6113684857283136</td>\n",
       "      <td>513185321116836</td>\n",
       "      <td>1289090035210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>196378490547121</td>\n",
       "      <td>384595713876544</td>\n",
       "      <td>3465764145889081</td>\n",
       "      <td>7138363746590244</td>\n",
       "      <td>2037127152357025</td>\n",
       "      <td>69772274880400</td>\n",
       "      <td>62955909394576</td>\n",
       "      <td>8010592788667225</td>\n",
       "      <td>12669869394576</td>\n",
       "      <td>1180895698690569</td>\n",
       "      <td>...</td>\n",
       "      <td>90766015494400</td>\n",
       "      <td>775019887394041</td>\n",
       "      <td>5609292930315369</td>\n",
       "      <td>1449463478112400</td>\n",
       "      <td>5197174711993489</td>\n",
       "      <td>17178894536049</td>\n",
       "      <td>234544201706896</td>\n",
       "      <td>39165355117729</td>\n",
       "      <td>2048348658446596</td>\n",
       "      <td>4330366199248324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2961342014949796</td>\n",
       "      <td>678613180542025</td>\n",
       "      <td>4530780144544900</td>\n",
       "      <td>6008298034556944</td>\n",
       "      <td>1743870706663684</td>\n",
       "      <td>5581324261644121</td>\n",
       "      <td>329371536771216</td>\n",
       "      <td>1148995490810896</td>\n",
       "      <td>7708482306949369</td>\n",
       "      <td>2208631159397776</td>\n",
       "      <td>...</td>\n",
       "      <td>730305169657281</td>\n",
       "      <td>320143452857809</td>\n",
       "      <td>7351274721180625</td>\n",
       "      <td>3698201265614244</td>\n",
       "      <td>9472348087524</td>\n",
       "      <td>1782668910928896</td>\n",
       "      <td>6118404224812441</td>\n",
       "      <td>6129201498473089</td>\n",
       "      <td>3095992769589316</td>\n",
       "      <td>52856942197284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>3977470202227344</td>\n",
       "      <td>107320835618929</td>\n",
       "      <td>830945279844</td>\n",
       "      <td>3086961604026624</td>\n",
       "      <td>4711842366265321</td>\n",
       "      <td>1437973421184400</td>\n",
       "      <td>234800795350521</td>\n",
       "      <td>1743325367909904</td>\n",
       "      <td>1327628297249956</td>\n",
       "      <td>587601694807009</td>\n",
       "      <td>...</td>\n",
       "      <td>953331328132096</td>\n",
       "      <td>669732526814481</td>\n",
       "      <td>9854895040286976</td>\n",
       "      <td>1790783162981316</td>\n",
       "      <td>1704794320614025</td>\n",
       "      <td>360751712706225</td>\n",
       "      <td>988775384150401</td>\n",
       "      <td>1905336031524025</td>\n",
       "      <td>197825434591849</td>\n",
       "      <td>713561075454096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9681044108936209</td>\n",
       "      <td>4149578747866176</td>\n",
       "      <td>4325071527356176</td>\n",
       "      <td>1435481296960516</td>\n",
       "      <td>7235306182723609</td>\n",
       "      <td>3939001614469156</td>\n",
       "      <td>1607840782452100</td>\n",
       "      <td>7099328833975225</td>\n",
       "      <td>7246265795250001</td>\n",
       "      <td>1498215282449796</td>\n",
       "      <td>...</td>\n",
       "      <td>2861158704040000</td>\n",
       "      <td>6257511636669225</td>\n",
       "      <td>43248576613225</td>\n",
       "      <td>4326478360992841</td>\n",
       "      <td>4780460457483876</td>\n",
       "      <td>7337080876324644</td>\n",
       "      <td>5240858948325729</td>\n",
       "      <td>2123257836907684</td>\n",
       "      <td>5601608020019881</td>\n",
       "      <td>811940120560969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                 1                 2                 3    \\\n",
       "0     8861896394421316  2516512275852201   372733605816100  4172308873801744   \n",
       "1     9155879298676881  6709341313903761    65115732802624   890036469243441   \n",
       "2     3498268242745600   669675749072836  8038721756248561  1850330143135296   \n",
       "3        5311420841104    26637211265625  1822132330619364  5192115845970244   \n",
       "4     7723203564934081   666917970810025   548547784683409  6585170425777969   \n",
       "...                ...               ...               ...               ...   \n",
       "9995  5766018068358601    99489433364721  1637230981469521  4248013433289796   \n",
       "9996   196378490547121   384595713876544  3465764145889081  7138363746590244   \n",
       "9997  2961342014949796   678613180542025  4530780144544900  6008298034556944   \n",
       "9998  3977470202227344   107320835618929      830945279844  3086961604026624   \n",
       "9999  9681044108936209  4149578747866176  4325071527356176  1435481296960516   \n",
       "\n",
       "                   4                 5                 6                 7    \\\n",
       "0     5427823796622756   762071635866025   292111749490729  9997616142086400   \n",
       "1     9545029142782225   747917698471056  1806302955388129  1476837230529600   \n",
       "2     1072158795994896    28563637494016  1021796322871401  5273630994852900   \n",
       "3     5280184349425129   363343527111184  1167735767278201  6401713074618436   \n",
       "4         147144356836  1168733602068516  7887804921289225  2172900609241924   \n",
       "...                ...               ...               ...               ...   \n",
       "9995    56538564139369    45277368491716   259256820914116  4111923029021284   \n",
       "9996  2037127152357025    69772274880400    62955909394576  8010592788667225   \n",
       "9997  1743870706663684  5581324261644121   329371536771216  1148995490810896   \n",
       "9998  4711842366265321  1437973421184400   234800795350521  1743325367909904   \n",
       "9999  7235306182723609  3939001614469156  1607840782452100  7099328833975225   \n",
       "\n",
       "                   8                 9    ...               990  \\\n",
       "0     6321334108485124  1271447821372689  ...   251821352454400   \n",
       "1     3410535705643264  4293490110523441  ...  4681045028294569   \n",
       "2     2872036653400996  6139705831875625  ...   509823885318400   \n",
       "3     1904908371305536  1169945083891600  ...  8993614065000889   \n",
       "4     2184455880614569  9670344739843921  ...   106726420399104   \n",
       "...                ...               ...  ...               ...   \n",
       "9995  9518272623873225  3034494608545009  ...  1205378755659225   \n",
       "9996    12669869394576  1180895698690569  ...    90766015494400   \n",
       "9997  7708482306949369  2208631159397776  ...   730305169657281   \n",
       "9998  1327628297249956   587601694807009  ...   953331328132096   \n",
       "9999  7246265795250001  1498215282449796  ...  2861158704040000   \n",
       "\n",
       "                   991               992               993               994  \\\n",
       "0     2339752576838121     3704089009201  3943910838718096  6966029135233081   \n",
       "1     2865903717266496  1946334394598400  3823240248510736  2004459901728025   \n",
       "2      916343854253584  3164205151607824  5673962587004100   712247208961600   \n",
       "3     4729322665345041  4174562196165904  1025072216529001    40104241849681   \n",
       "4     1490704223710321  2282394441945744  3051335410343236  1148400881668096   \n",
       "...                ...               ...               ...               ...   \n",
       "9995   166931852284521  1725772410201889     7003729359936   166618090411969   \n",
       "9996   775019887394041  5609292930315369  1449463478112400  5197174711993489   \n",
       "9997   320143452857809  7351274721180625  3698201265614244     9472348087524   \n",
       "9998   669732526814481  9854895040286976  1790783162981316  1704794320614025   \n",
       "9999  6257511636669225    43248576613225  4326478360992841  4780460457483876   \n",
       "\n",
       "                   995               996               997               998  \\\n",
       "0     1289798731957824  1984177646603881   300951681465681  7736057397619600   \n",
       "1     7188269002995600  2243874077724304  6803153467275361  4944864005354529   \n",
       "2       16249839769881  6765721667556049  5029166424891456    30716934105796   \n",
       "3     1644630079213521  4089657497184900  2461892336851600   319438981596736   \n",
       "4     5580157529004304   496791675302976      853373183524  5421407382862849   \n",
       "...                ...               ...               ...               ...   \n",
       "9995  7454134896878224  6223394012257081  6113684857283136   513185321116836   \n",
       "9996    17178894536049   234544201706896    39165355117729  2048348658446596   \n",
       "9997  1782668910928896  6118404224812441  6129201498473089  3095992769589316   \n",
       "9998   360751712706225   988775384150401  1905336031524025   197825434591849   \n",
       "9999  7337080876324644  5240858948325729  2123257836907684  5601608020019881   \n",
       "\n",
       "                   999  \n",
       "0     5319091648002609  \n",
       "1       19988598081321  \n",
       "2     9946115385025369  \n",
       "3      188127202106809  \n",
       "4     7177021597028416  \n",
       "...                ...  \n",
       "9995  1289090035210000  \n",
       "9996  4330366199248324  \n",
       "9997    52856942197284  \n",
       "9998   713561075454096  \n",
       "9999   811940120560969  \n",
       "\n",
       "[10000 rows x 1000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(0).progress_apply(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7157d1-721d-480f-83e4-ec1b1db7b405",
   "metadata": {},
   "source": [
    "# Improving dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47581af6-60f1-46fb-9372-c37a597f0c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccs': {'history': [[670.0], [670.0]],\n",
       "  'targets': [[670.0], []],\n",
       "  'extra_features': {'delta_days': [0.0, 162.0],\n",
       "   'date_last_history': ['2016-02-28', '2016-08-08']}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['0000676389D1EE60EB48AF5693F3F3DE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b169e01-01c3-43ae-8ce9-ee7ac05ee45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence, pack_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd7ea2-4e51-4013-aecc-e3d42fa16bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubsetRandomSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e31b25-bab6-4961-ae9c-a51cad3791ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "train_size = 0.70\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "test = 0.15*n\n",
    "train_size = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7e3c87-79b3-455b-b79f-de33cc74699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICareDataset_fast(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 diagnoses_file, \n",
    "                 universe_grouping, \n",
    "                 grouping='ccs', # desired grouping to use (for both input and output currently),\n",
    "                 train_size:float = 0.70,\n",
    "                 val_size:float = 0.15,\n",
    "                 test_size:float = 0.15,\n",
    "                 shuffle_dataset:bool = True,\n",
    "                 random_seed :int = 432\n",
    "                ):\n",
    "        \n",
    "        assert train_size+val_size+test_size == 1, 'Oops'\n",
    "\n",
    "        with open(diagnoses_file,'r') as fp:\n",
    "            self.raw_data = json.load(fp)\n",
    "\n",
    "        # list patients\n",
    "        self.patients = list(self.raw_data.keys())\n",
    "        \n",
    "        self.grouping = grouping\n",
    "        self.universe_grouping=universe_grouping\n",
    "        \n",
    "        self.__preprocess()\n",
    "        \n",
    "        self.data = {}\n",
    "        \n",
    "        print('processing each patient')\n",
    "        for pat in tqdm(self.raw_data):\n",
    "            \n",
    "            history_sequence = self.adms2multihot(self.raw_data[pat][self.grouping]['history'])\n",
    "            target_sequence = self.adms2multihot(self.raw_data[pat][self.grouping]['targets'])\n",
    "            \n",
    "            self.data[pat] = {'history_sequence':history_sequence,\n",
    "                              'target_sequence':target_sequence\n",
    "                             }\n",
    "        \n",
    "        dataset_size = len(self.patients)\n",
    "        indices = list(range(dataset_size))\n",
    "        if shuffle_dataset :\n",
    "            np.random.seed(random_seed)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "        train_split = int(np.floor(train_size * dataset_size))\n",
    "        val_split = int(np.floor(val_size * dataset_size))\n",
    "        \n",
    "        self.train_indices = indices[:train_split]\n",
    "        self.val_indices = indices[train_split:train_split+val_split]\n",
    "        self.test_indices = indices[-(train_split+val_split):]\n",
    "            \n",
    "            \n",
    "    def adms2multihot(self,adms):\n",
    "        #print(adms)\n",
    "        #print(self.grouping_data[self.grouping]['code2int'].keys())\n",
    "        return (torch.stack(\n",
    "                                [ F.one_hot( # list comprehension\n",
    "                                    # create a multi-hot of diagnoses of each admission\n",
    "                                     torch.tensor( \n",
    "                                         list(map(lambda code: self.grouping_data[self.grouping]['code2int'][code],\n",
    "                                             set(admission) # we don't care about repeated codes\n",
    "                                            ))\n",
    "                                     ),\n",
    "                                     num_classes=self.grouping_data[grouping]['n_labels']\n",
    "                                 )\n",
    "                                 .sum(dim=0)\n",
    "                                 .float()\n",
    "                                 if admission \n",
    "                                 else\n",
    "                                 torch.zeros(size=(self.grouping_data[grouping]['n_labels'],))\n",
    "                                 for admission in adms\n",
    "                                ]\n",
    "                            )\n",
    "               )\n",
    "    def __preprocess(self):\n",
    "        # necessary data of each code_grouping (eg. ccs, chapters) for posterior padding and one_hot_encoding of batches\n",
    "        self.grouping_data = {}\n",
    "        for grouping_code in self.raw_data[list(self.raw_data.keys())[0]].keys():\n",
    "            self.grouping_data[grouping_code] = {}\n",
    "\n",
    "            # get all codes of this group\n",
    "            all_data_grouping = self.universe_grouping\n",
    "\n",
    "            # store n_labels this group\n",
    "            self.grouping_data[grouping_code]['n_labels'] = len(set(all_data_grouping))\n",
    "\n",
    "            # store unique sorted codes from dataset\n",
    "            self.grouping_data[grouping_code]['sorted'] = sorted(set(all_data_grouping))\n",
    "\n",
    "            # store code2int & int2code\n",
    "            int2code = dict(enumerate(self.grouping_data[grouping_code]['sorted']))\n",
    "            code2int = {ch: ii for ii, ch in int2code.items()}\n",
    "\n",
    "            self.grouping_data[grouping_code]['int2code'] = int2code\n",
    "            self.grouping_data[grouping_code]['code2int'] = code2int\n",
    "            self.grouping_data[grouping_code]['int2code_converter'] = lambda idx: self.grouping_data[grouping_code]['int2code'][idx]\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Available groupings: ' +str(self.data[list(self.data.keys())[0]].keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        gets original converted from int2code\n",
    "        \"\"\"\n",
    "        patient_data = self.data[self.patients[idx]]\n",
    "\n",
    "\n",
    "        return {'train':patient_data['history_sequence'],\n",
    "                'target':patient_data['target_sequence'],\n",
    "                'pid':self.patients[idx]\n",
    "               }\n",
    "    \n",
    "    \n",
    "    \n",
    "class ICareCOLLATE_fast:\n",
    "    \"\"\"\n",
    "    This collate class gets a dataset in the format of:\n",
    "    [\n",
    "    {'train':[[[code1,code2],[code3]],[[etc..],[etc...]]]\n",
    "      'target:':[[[code1],[code2]],[[etc..],[etc...]]]\n",
    "    },\n",
    "     {etc..},\n",
    "     etc..\n",
    "    ]\n",
    "    \n",
    "    And outputs a pack of train and pad of test sequences\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,batch):\n",
    "        return {'train_sequences' : dict(sequence=pack_sequence([batch[i]['train'] for i in range(len(batch))],enforce_sorted=False)),\n",
    "                'target_sequences': dict(sequence=pad_sequence([batch[i]['target'] for i in range(len(batch))],batch_first=True)),\n",
    "                'pids': [e['pid'] for e in batch]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10d7b1e4-5294-4a03-be9a-159a2dc9d997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing each patient\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac30fd9e76f74e10804db85affe96ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ccs_universe = list(icdmap.icd9_3toccs.data.keys())\n",
    "dataset_folder = '/home/debian/Simao/master-thesis/data/model_ready_dataset/icare2021_diag_A301'\n",
    "dataset_fast = ICareDataset_dev(os.path.join(dataset_folder,'dataset.json'),ccs_universe,grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f70f5170-4d7a-4793-9753-b94b0ea1a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_fast = DataLoader(dataset_fast,batch_size=64,sampler=RandomSampler(dataset_fast.train_indices),collate_fn=ICareCOLLATE_dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee3ebdb0-ac4f-4bf4-8d55-9bb9d3f0dc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.18 s ± 734 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 1\n",
    "for batch in train_dataloader_fast:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24bf4b21-69d3-4bf3-b845-b8be947f0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 39s ± 32.5 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 1\n",
    "for batch in train_dataloader_slow:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "537b78a7-9599-4f6d-ba6b-b0dcd378aa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ola linda ❤️'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "714cd453-b07e-4b58-992e-23c52b9e0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4281b4bd-b942-497a-8d1d-83f5e98d1ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_____________ = 'ola'\n",
    "aa = 'ola'\n",
    "a = 'adeus'\n",
    "\n",
    "bbbbbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d41d3ec-4697-4f0d-aaaa-08318f8d17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset,\n",
    "           batch_size=batch_size,\n",
    "           collate_fn=ICareCOLLATE_dev(),\n",
    "           sampler=SubsetRandomSampler(dataset.train_indices)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2a5e645-6c0d-4218-9712-1e81384a1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,lengths = pad_packed_sequence(next(iter(train_dataloader))['train_sequences'],batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a26606b-b920-4357-a9c3-50a6961408b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  2,  7,  6, 21,  7,  6,  2, 20, 13,  2,  5, 19,  3, 12,  2, 11, 11,\n",
       "         3,  2, 10,  5,  5,  6,  9,  2,  2,  2,  2, 10,  2, 10,  2,  5, 17,  2,\n",
       "         3, 13,  2,  4,  3,  6,  7,  5,  2,  3,  2,  9, 10, 12, 10,  2,  3,  3,\n",
       "         2, 24,  1,  2, 21, 11,  5, 13,  8, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a8d2083-0e9f-476d-9bca-2254540dc3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183967"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing each patient\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c51db06348944468b8270bff8bb4ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183967 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "258.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n\u001b[1;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,collate_fn\u001b[38;5;241m=\u001b[39mICareCOLLATE(dataset),shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m train_dataset_dev \u001b[38;5;241m=\u001b[39m \u001b[43mICareDataset_dev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_subset.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrouping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mlen\u001b[39m(train_dataset_dev)\n\u001b[1;32m      7\u001b[0m train_dataloader_dev \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset_dev,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,collate_fn\u001b[38;5;241m=\u001b[39mICareCOLLATE_dev,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mICareDataset_dev.__init__\u001b[0;34m(self, diagnoses_file, universe_grouping, grouping)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessing each patient\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pat \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data):\n\u001b[0;32m---> 23\u001b[0m     history_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madms2multihot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     target_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madms2multihot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data[pat][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[pat] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m:history_sequence,\n\u001b[1;32m     27\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m:target_sequence\n\u001b[1;32m     28\u001b[0m                      }\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mICareDataset_dev.adms2multihot\u001b[0;34m(self, adms)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madms2multihot\u001b[39m(\u001b[38;5;28mself\u001b[39m,adms):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m---> 34\u001b[0m                             [ F\u001b[38;5;241m.\u001b[39mone_hot( \u001b[38;5;66;03m# list comprehension\u001b[39;00m\n\u001b[1;32m     35\u001b[0m                                 \u001b[38;5;66;03m# create a multi-hot of diagnoses of each admission\u001b[39;00m\n\u001b[1;32m     36\u001b[0m                                  torch\u001b[38;5;241m.\u001b[39mtensor( \n\u001b[1;32m     37\u001b[0m                                      \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m code: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_data[grouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode2int\u001b[39m\u001b[38;5;124m'\u001b[39m][code],\n\u001b[1;32m     38\u001b[0m                                          \u001b[38;5;28mset\u001b[39m(admission) \u001b[38;5;66;03m# we don't care about repeated codes\u001b[39;00m\n\u001b[1;32m     39\u001b[0m                                         ))\n\u001b[1;32m     40\u001b[0m                                  ),\n\u001b[1;32m     41\u001b[0m                                  num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_data[grouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m                              )\n\u001b[1;32m     43\u001b[0m                              \u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m                              \u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     45\u001b[0m                              \u001b[38;5;28;01mif\u001b[39;00m admission \n\u001b[1;32m     46\u001b[0m                              \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m     47\u001b[0m                              torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_data[grouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_labels\u001b[39m\u001b[38;5;124m'\u001b[39m],))\n\u001b[1;32m     48\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m admission \u001b[38;5;129;01min\u001b[39;00m adms\n\u001b[1;32m     49\u001b[0m                             ]\n\u001b[1;32m     50\u001b[0m                         )\n\u001b[1;32m     51\u001b[0m            )\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madms2multihot\u001b[39m(\u001b[38;5;28mself\u001b[39m,adms):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m     34\u001b[0m                             [ F\u001b[38;5;241m.\u001b[39mone_hot( \u001b[38;5;66;03m# list comprehension\u001b[39;00m\n\u001b[1;32m     35\u001b[0m                                 \u001b[38;5;66;03m# create a multi-hot of diagnoses of each admission\u001b[39;00m\n\u001b[1;32m     36\u001b[0m                                  torch\u001b[38;5;241m.\u001b[39mtensor( \n\u001b[0;32m---> 37\u001b[0m                                      \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouping_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrouping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode2int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madmission\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# we don't care about repeated codes\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m                                  ),\n\u001b[1;32m     41\u001b[0m                                  num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_data[grouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m                              )\n\u001b[1;32m     43\u001b[0m                              \u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m                              \u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     45\u001b[0m                              \u001b[38;5;28;01mif\u001b[39;00m admission \n\u001b[1;32m     46\u001b[0m                              \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m     47\u001b[0m                              torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_data[grouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_labels\u001b[39m\u001b[38;5;124m'\u001b[39m],))\n\u001b[1;32m     48\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m admission \u001b[38;5;129;01min\u001b[39;00m adms\n\u001b[1;32m     49\u001b[0m                             ]\n\u001b[1;32m     50\u001b[0m                         )\n\u001b[1;32m     51\u001b[0m            )\n",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36mICareDataset_dev.adms2multihot.<locals>.<lambda>\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madms2multihot\u001b[39m(\u001b[38;5;28mself\u001b[39m,adms):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m     34\u001b[0m                             [ F\u001b[38;5;241m.\u001b[39mone_hot( \u001b[38;5;66;03m# list comprehension\u001b[39;00m\n\u001b[1;32m     35\u001b[0m                                 \u001b[38;5;66;03m# create a multi-hot of diagnoses of each admission\u001b[39;00m\n\u001b[1;32m     36\u001b[0m                                  torch\u001b[38;5;241m.\u001b[39mtensor( \n\u001b[0;32m---> 37\u001b[0m                                      \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m code: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouping_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrouping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode2int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     38\u001b[0m                                          \u001b[38;5;28mset\u001b[39m(admission) \u001b[38;5;66;03m# we don't care about repeated codes\u001b[39;00m\n\u001b[1;32m     39\u001b[0m                                         ))\n\u001b[1;32m     40\u001b[0m                                  ),\n\u001b[1;32m     41\u001b[0m                                  num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_data[grouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m                              )\n\u001b[1;32m     43\u001b[0m                              \u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m                              \u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     45\u001b[0m                              \u001b[38;5;28;01mif\u001b[39;00m admission \n\u001b[1;32m     46\u001b[0m                              \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m     47\u001b[0m                              torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_data[grouping][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_labels\u001b[39m\u001b[38;5;124m'\u001b[39m],))\n\u001b[1;32m     48\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m admission \u001b[38;5;129;01min\u001b[39;00m adms\n\u001b[1;32m     49\u001b[0m                             ]\n\u001b[1;32m     50\u001b[0m                         )\n\u001b[1;32m     51\u001b[0m            )\n",
      "\u001b[0;31mKeyError\u001b[0m: 258.0"
     ]
    }
   ],
   "source": [
    "train_dataset = IcareDataset(os.path.join(dataset_folder,'train_subset.json'),grouping)\n",
    "len(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,collate_fn=ICareCOLLATE(dataset),shuffle=True)\n",
    "\n",
    "train_dataset_dev = ICareDataset_dev(os.path.join(dataset_folder,'train_subset.json'),grouping)\n",
    "len(train_dataset_dev)\n",
    "train_dataloader_dev = DataLoader(train_dataset_dev,batch_size=batch_size,collate_fn=ICareCOLLATE_dev,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cac75729-804b-4154-b4f3-2459df09a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing each patient\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35adfa95a2a642a8a6692db72c0d8047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ccs_universe = list(icdmap.icd9_3toccs.data.keys())\n",
    "dataset_folder = '/home/debian/Simao/master-thesis/data/model_ready_dataset/icare2021_diag_A301'\n",
    "dataset = IcareDataset_dev(os.path.join(dataset_folder,'dataset.json'),\n",
    "                           ccs_universe,\n",
    "                       grouping\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b860813-65b6-48f3-aced-2e1c1bfe137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [dataset[i]['train'] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "746cc0dd-c1fa-470e-a511-6b75fe849ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 283])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(batch,batch_first=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0db02c6f-173f-4ed9-a5e0-d4a47b1efca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'target': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'pid': '0000676389D1EE60EB48AF5693F3F3DE'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf736f1-753e-4b9f-b142-271ecddebf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bff0e8be-5010-4a7a-8444-f35f97264e5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 283] at entry 0 and [4, 283] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 283] at entry 0 and [4, 283] at entry 1"
     ]
    }
   ],
   "source": [
    "torch.stack([e['train'] for e in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105d3a1-1ff3-4fa5-b16d-e122bb0d6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICareCOLLATE_dev:\n",
    "    \"\"\"\n",
    "    This collate class gets a dataset in the format of:\n",
    "    [\n",
    "    {'train':[[[code1,code2],[code3]],[[etc..],[etc...]]]\n",
    "      'target:':[[[code1],[code2]],[[etc..],[etc...]]]\n",
    "    },\n",
    "     {etc..},\n",
    "     etc..\n",
    "    ]\n",
    "    \n",
    "    And outputs a pack of train and pad of test sequences\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,batch):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a58a81e-6fb5-48cf-8721-3dc7fc7d602f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000676389D1EE60EB48AF5693F3F3DE'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.data.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9df17dc3-e4ec-4b61-8fdd-724e1ab00aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000676389D1EE60EB48AF5693F3F3DE'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.patients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84a704da-26b6-4658-9687-a7f6deabcfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'target': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'pid': '0000676389D1EE60EB48AF5693F3F3DE'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f499353f-eb58-4bad-8dde-8b4711f7f0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000676389D1EE60EB48AF5693F3F3DE'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.raw_data.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c503fc7-7782-46d9-8599-e843ae93cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IcareDataset(Dataset):\n",
    "    def __init__(self, diagnoses_file, universe_grouping, grouping='ccs' # desired grouping to use (for both input and output currently),\n",
    "                ):\n",
    "        \n",
    "        # load admissions data\n",
    "        with open(diagnoses_file,'r') as fp:\n",
    "            self.data = json.load(fp)\n",
    "        \n",
    "        # list patients\n",
    "        self.patients = list(self.data.keys())\n",
    "        \n",
    "        self.grouping = grouping\n",
    "        self.universe_grouping=universe_grouping\n",
    "        \n",
    "        # create mappings between codes to one-hot into self.grouping_data\n",
    "        self.__preprocess()\n",
    "            \n",
    "    def __preprocess(self):\n",
    "        # necessary data of each code_grouping (eg. ccs, chapters) for posterior padding and one_hot_encoding of batches\n",
    "        self.grouping_data = {}\n",
    "        for grouping_code in self.data[list(self.data.keys())[0]].keys():\n",
    "            self.grouping_data[grouping_code] = {}\n",
    "            \n",
    "            # get all codes of this group\n",
    "            all_data_grouping = self.universe_grouping\n",
    "            \n",
    "            # store n_labels this group\n",
    "            self.grouping_data[grouping_code]['n_labels'] = len(set(all_data_grouping))\n",
    "            \n",
    "            # store unique sorted codes from dataset\n",
    "            self.grouping_data[grouping_code]['sorted'] = sorted(set(all_data_grouping))\n",
    "            \n",
    "            # store code2int & int2code\n",
    "            int2code = dict(enumerate(self.grouping_data[grouping_code]['sorted']))\n",
    "            code2int = {ch: ii for ii, ch in int2code.items()}\n",
    "            \n",
    "            self.grouping_data[grouping_code]['int2code'] = int2code\n",
    "            self.grouping_data[grouping_code]['code2int'] = code2int\n",
    "            self.grouping_data[grouping_code]['int2code_converter'] = lambda idx: self.grouping_data[grouping_code]['int2code'][idx]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Available groupings: ' +str(self.data[list(self.data.keys())[0]].keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        gets original converted from int2code\n",
    "        \"\"\"\n",
    "        patient_data = self.data[self.patients[idx]][self.grouping]\n",
    "        \n",
    "        train = patient_data['history']\n",
    "        target = patient_data['targets']\n",
    "        \n",
    "        # remove duplicates (can happen in low granuality codes such as ccs)\n",
    "        train = [list(set(admission)) for admission in train]\n",
    "        target = [list(set(admission)) for admission in target]\n",
    "        \n",
    "        return {'train':train,\n",
    "                'target':target,\n",
    "                'pid':self.patients[idx]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2470396-c83f-466c-83e2-f61d39cc1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICareCOLLATE:\n",
    "    \"\"\"\n",
    "    This collate class gets a dataset in the format of:\n",
    "    [\n",
    "    {'train':[[[code1,code2],[code3]],[[etc..],[etc...]]]\n",
    "      'target:':[[[code1],[code2]],[[etc..],[etc...]]]\n",
    "    },\n",
    "     {etc..},\n",
    "     etc..\n",
    "    ]\n",
    "    \n",
    "    And outputs a pack of train and pad of test sequences\n",
    "    \"\"\"\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __call__(self,batch):\n",
    "        patients = {'train':{'sequence':[],'original':[],'pids':[]},\n",
    "                    'target':{'sequence':[],'original':[],'pids':[]}\n",
    "                   }\n",
    "        \n",
    "        grouping_code = self.dataset.grouping\n",
    "        n_labels = self.dataset.grouping_data[grouping_code]['n_labels']\n",
    "        code2int = self.dataset.grouping_data[grouping_code]['code2int']\n",
    "        \n",
    "        # <Nº admissions - 1> of each patient\n",
    "        seq_lengths = []\n",
    "        \n",
    "        # 1-to-1 correspondence between each admission in {train/target}_admissions_sequenced and the patient's id.\n",
    "        patients_list = []\n",
    "        for pat in batch:\n",
    "            \n",
    "            pid = pat['pid'] # patient id\n",
    "            train_admissions_sequenced = []\n",
    "            target_admissions_sequenced = []\n",
    "            seq_lengths.append(len(pat['train']))\n",
    "\n",
    "            # convert each train admission into a multi-hot vector\n",
    "            for train_admission in pat['train']:\n",
    "                admission = (F.one_hot(torch.tensor(list(map(lambda code: code2int[code],train_admission))),num_classes=n_labels)\n",
    "                             .sum(dim=0).float() #one-hot of each diagnose to multi-hot vector of diagnoses\n",
    "                            )\n",
    "                train_admissions_sequenced.append(admission)\n",
    "            \n",
    "            \n",
    "\n",
    "            # convert each target admission into a one-hot vector\n",
    "            for target_admission in pat['target']:\n",
    "                \n",
    "                if not target_admission: # target is empty\n",
    "                    admission = torch.zeros(size=(n_labels,))\n",
    "                else: #target has at least 1 diagnostic\n",
    "                    # convert admission to multi-hot vector\n",
    "                    admission = (F.one_hot(torch.tensor(list(map(lambda code: code2int[code],target_admission))),num_classes=n_labels)\n",
    "                                 .sum(dim=0).float() #one-hot of each diagnose to multi-hot vector of diagnoses\n",
    "                                )\n",
    "                target_admissions_sequenced.append(admission)\n",
    "\n",
    "            # stack multiple train admissions of a single patient into a single tensor\n",
    "            if len(train_admissions_sequenced) > 1:\n",
    "                train_admissions_sequenced = torch.stack(train_admissions_sequenced)\n",
    "            else:\n",
    "                train_admissions_sequenced = train_admissions_sequenced[0].view((1,-1))\n",
    "\n",
    "            # stack multiple target admissions of a single patient into a single tensor\n",
    "            if len(target_admissions_sequenced) > 1:\n",
    "                target_admissions_sequenced = torch.stack(target_admissions_sequenced)\n",
    "            else:\n",
    "                target_admissions_sequenced = target_admissions_sequenced[0].view((1,-1))\n",
    "\n",
    "            # store final train and test tensors\n",
    "            patients['train']['sequence'].append(train_admissions_sequenced)\n",
    "            patients['target']['sequence'].append(target_admissions_sequenced)\n",
    "            \n",
    "            patients['train']['original'].append(pat['train'])\n",
    "            patients['target']['original'].append(pat['target'])\n",
    "            \n",
    "            # repeat pid for each admission they have on target\n",
    "            pid_train_list = [pid] * len(pat['train'])\n",
    "            pid_target_list = [pid] * len(pat['target'])\n",
    "            patients['train']['pids'].extend(pid_train_list)\n",
    "            patients['target']['pids'].extend(pid_target_list)\n",
    "\n",
    "        # pad sequences (some patients have more admissions than others)\n",
    "        patients['train']['sequence'] = pack_sequence(patients['train']['sequence'],enforce_sorted=False)\n",
    "        patients['target']['sequence'] = pad_sequence(patients['target']['sequence'],batch_first=True)\n",
    "        \n",
    "        return {'train_sequences':patients['train'],\n",
    "                'target_sequences':patients['target'],\n",
    "                'train_pids':patients['train']['pids'],\n",
    "                'target_pids':patients['target']['pids']\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861b0a8-e7be-4d73-9352-4951e2cc1392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (simao thesis)",
   "language": "python",
   "name": "simao_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
