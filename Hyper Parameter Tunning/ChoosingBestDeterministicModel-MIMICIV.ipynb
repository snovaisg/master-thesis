{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# protection against running this cell multiple times\n",
    "assert os.path.dirname(cwd).split('/')[-1] == 'master-thesis','Oops, directory already changed previously as indended. Ignoring...'\n",
    "\n",
    "# change working directory (if assert passed)\n",
    "new_cwd = os.path.dirname(cwd) # parent directory\n",
    "os.chdir(new_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rnn_utils import DiagnosesDataset, split_dataset, MYCOLLATE\n",
    "from rnn_utils import RNN, train_one_epoch, eval_model, compute_loss\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from config import Settings; settings = Settings()\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'diag_only/mimic_iv_quick_baseline_dataset'\n",
    "\n",
    "# model\n",
    "grouping = 'ccs'\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd057965cb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducibility\n",
    "seed = settings.random_seed\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset at data/model_ready_dataset/diag_only/mimic_iv_quick_baseline_dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = os.path.join(settings.data_base,settings.model_ready_dataset_folder,dataset_id)\n",
    "print('dataset at',dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38837"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8323"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8323"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DiagnosesDataset(os.path.join(dataset_folder,'mimic_iv_quick_baseline_dataset.json'),grouping)\n",
    "\n",
    "train_dataset = DiagnosesDataset(os.path.join(dataset_folder,'train_subset.json'),grouping)\n",
    "val_dataset = DiagnosesDataset(os.path.join(dataset_folder,'val_subset.json'),grouping)\n",
    "test_dataset = DiagnosesDataset(os.path.join(dataset_folder,'test_subset.json'),grouping)\n",
    "\n",
    "\n",
    "len(train_dataset)\n",
    "len(val_dataset)\n",
    "len(test_dataset)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,collate_fn=MYCOLLATE(dataset),shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,collate_fn=MYCOLLATE(dataset)) #batch_size here is arbitrary and doesn't affect total validation speed\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,collate_fn=MYCOLLATE(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, lenghts = pad_packed_sequence(next(iter(train_dataloader))['train_sequences']['sequence'],batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnovaisg\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data and train conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = input_size = next(iter(train_dataloader))['target_sequences']['sequence'].shape[2]\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 27\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'hidden_size':[50,75,100],\n",
    "    'num_layers':[2],\n",
    "    'lr':[0.01,0.02,0.03],\n",
    "    'model':['rnn','gru','lstm']\n",
    "    \n",
    "}\n",
    "meta_parameters = {\n",
    "    'epochs':10\n",
    "}\n",
    "\n",
    "params = ParameterGrid(hyperparameters)\n",
    "print(f'params:',len(params))\n",
    "\n",
    "#random_params = ParameterSampler(params.param_grid,n_iter=len(params)-1,random_state=231)\n",
    "#next(iter(random_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataloader))['train_sequences']['sequence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]]), batch_sizes=tensor([64, 33, 25, 17, 14, 10,  8,  7,  6,  4,  4,  3,  2,  2,  2,  2,  2,  2,\n",
       "         2,  1]), sorted_indices=tensor([ 7, 15, 13,  1, 12, 52, 19, 16, 22,  2, 48, 46, 43,  3, 27, 17, 18, 24,\n",
       "        51, 28, 31, 63, 33, 41,  8, 26, 21, 49, 30,  9, 32, 50, 53, 57,  6,  5,\n",
       "         4, 54, 55, 56, 44, 58, 59, 60, 61, 62,  0, 37, 25, 20, 29, 14, 11, 34,\n",
       "        35, 36, 47, 38, 39, 40, 10, 42, 23, 45]), unsorted_indices=tensor([46,  3,  9, 13, 36, 35, 34,  0, 24, 29, 60, 52,  4,  2, 51,  1,  7, 15,\n",
       "        16,  6, 49, 26,  8, 62, 17, 48, 25, 14, 19, 50, 28, 20, 30, 22, 53, 54,\n",
       "        55, 47, 57, 58, 59, 23, 61, 12, 40, 63, 11, 56, 10, 27, 31, 18,  5, 32,\n",
       "        37, 38, 39, 33, 41, 42, 43, 44, 45, 21]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64, 33, 25, 17, 14, 10,  8,  7,  6,  4,  4,  3,  2,  2,  2,  2,  2,  2,\n",
       "         2,  1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fd04ccaa4a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/debian/Simao/master-thesis/wandb/run-20220606_173823-pver3qoe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/snovaisg/baseline_quick_mimicIV/runs/pver3qoe\" target=\"_blank\">chocolate-butterfly-1</a></strong> to <a href=\"https://wandb.ai/snovaisg/baseline_quick_mimicIV\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/snovaisg/baseline_quick_mimicIV/runs/pver3qoe?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd00d12ca00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "TypeError",
     "evalue": "eval_model() missing 1 required positional argument: 'decision_thresholds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model,train_dataloader,epoch,criterion,optimizer);\n\u001b[1;32m     23\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:epoch,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m:loss})\n\u001b[0;32m---> 25\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfilter(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_adm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m eval_model(model,val_dataloader,dataset,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfilter(regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_adm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m train_metrics\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m train_metrics\u001b[38;5;241m.\u001b[39mindex]\n",
      "\u001b[0;31mTypeError\u001b[0m: eval_model() missing 1 required positional argument: 'decision_thresholds'"
     ]
    }
   ],
   "source": [
    "for idx,param_set in enumerate(params):\n",
    "    config = {**param_set, \n",
    "              **meta_parameters}\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"baseline_quick_mimicIV\", \n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    model = RNN(input_size=input_size,\n",
    "              hidden_size=config['hidden_size'],\n",
    "              num_layers=config['num_layers'],\n",
    "              n_labels=n_labels,\n",
    "              model=config['model'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    for epoch in range(1,config['epochs']+1):\n",
    "        log = {}\n",
    "        \n",
    "        loss = train_one_epoch(model,train_dataloader,epoch,criterion,optimizer);\n",
    "        \n",
    "        wandb.log({'epoch':epoch,'loss':loss})\n",
    "        \n",
    "    train_metrics = eval_model(model,train_dataloader,dataset,metrics=['roc','f1'])[1].filter(regex='_adm')\n",
    "    val_metrics = eval_model(model,val_dataloader,dataset,metrics=['roc','f1'])[1].filter(regex='_adm')\n",
    "    train_metrics.index = ['train_' + n for n in train_metrics.index]\n",
    "    val_metrics.index = ['val_' + n for n in val_metrics.index]\n",
    "        \n",
    "\n",
    "    log = dict()\n",
    "\n",
    "    log.update(train_metrics.to_dict())\n",
    "    log.update(val_metrics.to_dict())\n",
    "    log.update({'loss':loss})\n",
    "\n",
    "    wandb.log(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best model\n",
    "\n",
    "- name: silver-salad-21\n",
    "- lr: 0.01\n",
    "- model_type = 'lstm'\n",
    "- num_layers = 1\n",
    "- hidden_size = 75\n",
    "- epochs: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'hidden_size':[25,50,75],\n",
    "    'num_layers':[2],\n",
    "    'lr':[0.01,0.02,0.03],\n",
    "    'model':['rnn','gru','lstm']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 15}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fv6qbc51) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">royal-dust-52</strong>: <a href=\"https://wandb.ai/snovaisg/basic_deterministic_model_tunning/runs/fv6qbc51\" target=\"_blank\">https://wandb.ai/snovaisg/basic_deterministic_model_tunning/runs/fv6qbc51</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220421_175909-fv6qbc51/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fv6qbc51). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/simaonovais/Documents/GitHub/master-thesis/wandb/run-20220421_175950-35ueo6uc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/snovaisg/basic_deterministic_model_tunning/runs/35ueo6uc\" target=\"_blank\">glamorous-dragon-53</a></strong> to <a href=\"https://wandb.ai/snovaisg/basic_deterministic_model_tunning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/snovaisg/basic_deterministic_model_tunning/runs/35ueo6uc?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x13939c9a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_set = dict(hidden_size=75,num_layers=1,lr=0.01,model_type='lstm',epochs=15,mode='metrics')\n",
    "wandb.init(\n",
    "    project=\"basic_deterministic_model_tunning\", \n",
    "    config=param_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=input_size,\n",
    "            hidden_size=param_set['hidden_size'],\n",
    "            num_layers=param_set['num_layers'],\n",
    "            n_labels=n_labels,\n",
    "            model=param_set['model_type'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=param_set['lr'])\n",
    "for epoch in range(1,config['epochs']+1):\n",
    "    loss = train_one_epoch(model,train_dataloader,epoch,criterion,optimizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {**param_set, \n",
    "              **meta_parameters}\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"basic_deterministic_model_tunning\", \n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    model = RNN(input_size=input_size,\n",
    "              hidden_size=config['hidden_size'],\n",
    "              num_layers=config['num_layers'],\n",
    "              n_labels=n_labels,\n",
    "              model=config['model'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    for epoch in range(1,config['epochs']+1):\n",
    "        log = {}\n",
    "        \n",
    "        loss = train_one_epoch(model,train_dataloader,epoch,criterion,optimizer);\n",
    "        \n",
    "        wandb.log({'epoch':epoch,'loss':loss})\n",
    "        \n",
    "    train_metrics = eval_model(model,train_dataloader,dataset,metrics=['roc','f1'])[1].filter(regex='_adm')\n",
    "    val_metrics = eval_model(model,val_dataloader,dataset,metrics=['roc','f1'])[1].filter(regex='_adm')\n",
    "    train_metrics.index = ['train_' + n for n in train_metrics.index]\n",
    "    val_metrics.index = ['val_' + n for n in val_metrics.index]\n",
    "        \n",
    "\n",
    "    log = dict()\n",
    "\n",
    "    log.update(train_metrics.to_dict())\n",
    "    log.update(val_metrics.to_dict())\n",
    "    log.update({'loss':loss})\n",
    "\n",
    "    wandb.log(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (simao thesis)",
   "language": "python",
   "name": "simao_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
