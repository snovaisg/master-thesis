{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "897f9d2a-1751-4539-8a01-acf40d09bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting split_patients.py\n"
     ]
    }
   ],
   "source": [
    "%%file split_patients.py\n",
    "#! /home/debian/Simao/miniconda3/envs/thesis/bin/python3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# protection against running this cell multiple times\n",
    "assert os.path.dirname(cwd).split('/')[-1] == 'master-thesis','Oops, directory already changed previously as indended. Ignoring...'\n",
    "\n",
    "new_cwd = os.path.dirname(cwd) # parent directory\n",
    "sys.path.append(new_cwd)\n",
    "\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from os.path import basename\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "from rnn_utils import DiagnosesDataset, MYCOLLATE, split_dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Split patients into train-test-validation')\n",
    "    parser.add_argument('-i','--input_file', type=str, help='file with patient ids')\n",
    "    parser.add_argument('-o','--output_path', type=str, help='path to save the splits')\n",
    "    parser.add_argument('-v','--val_size', type=float, help='size of the validation set in fraction')\n",
    "    parser.add_argument('-t','--test_size', type=float, help='size of the test set in fraction')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args.input_file)\n",
    "    print(args.output_path)\n",
    "    print(args.val_size)\n",
    "    print(args.test_size)\n",
    "    val_size = args.val_size\n",
    "    test_size = args.test_size\n",
    "    \n",
    "    assert os.path.isdir(args.output_path), 'Output path doesnt exist'\n",
    "    \n",
    "    seed = 426\n",
    "    print(f'{seed=}')\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    train_size = 1 - (test_size + val_size)\n",
    "    assert test_size + val_size < 1, 'Oops'\n",
    "    print('train_size=',train_size)\n",
    "    \n",
    "\n",
    "    with open(args.input_file,'r') as fp:\n",
    "        dataset = json.load(fp)\n",
    "    \n",
    "    pat_ids = list(dataset.keys())\n",
    "\n",
    "    whole_train,test = train_test_split(pat_ids,test_size=test_size)\n",
    "\n",
    "    val_size_corrected = val_size/(1-test_size)\n",
    "    train,val = train_test_split(whole_train,test_size=val_size_corrected)\n",
    "\n",
    "    print(f\"{len(whole_train)=}\")\n",
    "    print(f\"{len(train)=}\")\n",
    "    print(f\"{len(val)=}\")\n",
    "    print(f\"{len(test)=}\")\n",
    "    \n",
    "    def generate_subset_data(original,inclusion_list):\n",
    "        df = pd.DataFrame(original)\n",
    "        subset_original = df.loc[:,inclusion_list].to_dict()\n",
    "        return subset_original\n",
    "\n",
    "    whole_train_subset = generate_subset_data(dataset,whole_train)\n",
    "    train_subset = generate_subset_data(dataset,train)\n",
    "    val_subset = generate_subset_data(dataset,val)\n",
    "    test_subset = generate_subset_data(dataset,test)\n",
    "\n",
    "    # sanity checks\n",
    "\n",
    "    print(f\"{len(whole_train_subset)=}\")\n",
    "    print(f\"{len(train_subset)=}\")\n",
    "    print(f\"{len(val_subset)=}\")\n",
    "    print(f\"{len(test_subset)=}\")\n",
    "\n",
    "    # file suffix with metadata\n",
    "    params = {'train':train_size,\n",
    "              'eval':val_size,\n",
    "              'test':test_size,\n",
    "              'rseed':seed,\n",
    "             }\n",
    "    \n",
    "        # assign filename to each subset\n",
    "    names = {'whole_train_subset':whole_train_subset,\n",
    "             'train_subset':train_subset,\n",
    "             'val_subset':val_subset,\n",
    "             'test_subset':test_subset\n",
    "            }\n",
    "\n",
    "    # Save (finally!)\n",
    "    for name in names:\n",
    "\n",
    "        filename = os.path.join(args.output_path,name)\n",
    "        with open(filename+'.json','w') as fp:\n",
    "            json.dump(names[name],fp)\n",
    "\n",
    "    with open(os.path.join(args.output_path,'splits_metadata.json'),'w') as fp:\n",
    "        json.dump(params,fp)\n",
    "        \n",
    "    print('Done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b5c187a-960c-4801-9ca1-02a2a7adc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x split_patients.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97670f52-f0f3-408c-bc7e-e789d6759794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/debian/Simao/master-thesis/Setup\n",
      "/home/debian/Simao/master-thesis/data/model_ready_dataset/icare2021_diag_A301/dataset.json\n",
      "/home/debian/Simao/master-thesis/data/model_ready_dataset/icare2021_diag_A301\n",
      "0.15\n",
      "0.15\n",
      "seed=426\n",
      "train_size= 0.7\n",
      "len(whole_train)=223389\n",
      "len(train)=183967\n",
      "len(val)=39422\n",
      "len(test)=39422\n",
      "len(whole_train_subset)=223389\n",
      "len(train_subset)=183967\n",
      "len(val_subset)=39422\n",
      "len(test_subset)=39422\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!./split_patients.py -i '/home/debian/Simao/master-thesis/data/model_ready_dataset/icare2021_diag_A301/dataset.json' -o '/home/debian/Simao/master-thesis/data/model_ready_dataset/icare2021_diag_A301' --val_size 0.15 --test_size 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfffd71-fa94-4792-826f-13753ed75694",
   "metadata": {},
   "source": [
    "# Notebook objective: create and store train-validation-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1710d7bd-d3cd-4ef7-9fa3-2ddf96fafbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# protection against running this cell multiple times\n",
    "assert os.path.dirname(cwd).split('/')[-1] == 'master-thesis','Oops, directory already changed previously as indended. Ignoring...'\n",
    "\n",
    "# change working directory (if assert passed)\n",
    "new_cwd = os.path.dirname(cwd) # parent directory\n",
    "os.chdir(new_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9fdeff-820b-4ced-af58-ed4bdfb19f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f661e7-d96c-4d0a-911a-12ff877aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from os.path import basename\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from config import Settings; settings = Settings()\n",
    "import pandas as pd\n",
    "\n",
    "from rnn_utils import DiagnosesDataset, MYCOLLATE, split_dataset\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5aa9cf-fb9c-4f63-ba5b-c91e584c2b9f",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "478705f7-024a-4b40-818a-9e3c230ddbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f51c0c20590>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = settings.random_seed; print(f'{seed=}')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e79eec-b170-4184-b43f-fad69fa0b5a7",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Change these parameters as you prefer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6886dd97-32d1-4906-9612-02f23d80ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'diag_only'\n",
    "dataset_name = 'dataset'\n",
    "test_size = 0.15\n",
    "val_size=0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d964fe-f88d-4ada-b973-749a9382130d",
   "metadata": {},
   "source": [
    "sanity check and some processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f5348ad-9d40-4556-81f4-5a537facf32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1 - (test_size + val_size)\n",
    "assert test_size + val_size < 1, 'Oops'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7493e1-5363-41c6-b516-c6600bf2d4ff",
   "metadata": {},
   "source": [
    "## Create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bea42baf-f169-47b6-bcdd-293371ba04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(settings.data_base,settings.model_ready_dataset_folder,dataset_id)\n",
    "dataset_filepath = os.path.join(dataset_folder,dataset_name+'.json')\n",
    "\n",
    "with open(dataset_filepath,'r') as fp:\n",
    "    dataset = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10ce8a7e-fe45-4f98-8c33-dacb68382a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/model_ready_dataset/diag_only/dataset.json'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76458b28-1377-4275-84bd-169c6de5d7a2",
   "metadata": {},
   "source": [
    "split ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba73e6b-1916-413c-b663-bb778274cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(whole_train)=6374\n",
      "len(train)=5249\n",
      "len(val)=1125\n",
      "len(test)=1125\n"
     ]
    }
   ],
   "source": [
    "pat_ids = list(dataset['data'].keys())\n",
    "\n",
    "whole_train,test = train_test_split(pat_ids,test_size=test_size)\n",
    "\n",
    "val_size_corrected = val_size/(1-test_size)\n",
    "train,val = train_test_split(whole_train,test_size=val_size_corrected)\n",
    "\n",
    "print(f\"{len(whole_train)=}\")\n",
    "print(f\"{len(train)=}\")\n",
    "print(f\"{len(val)=}\")\n",
    "print(f\"{len(test)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d2a0f-bc96-4138-a176-5da7e466c679",
   "metadata": {},
   "source": [
    "create dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac877a8c-8602-4c35-a807-83c1647129a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(whole_train_subset['data'])=6374\n",
      "len(train_subset['data'])=5249\n",
      "len(val_subset['data'])=1125\n",
      "len(test_subset['data'])=1125\n"
     ]
    }
   ],
   "source": [
    "def generate_subset_data(original,inclusion_list):\n",
    "    df = pd.DataFrame(original['data'])\n",
    "    subset_original = df.loc[:,inclusion_list].to_dict()\n",
    "    subset_original = {'metadata':original['metadata'],'data':subset_original}\n",
    "    return subset_original\n",
    "\n",
    "whole_train_subset = generate_subset_data(dataset,whole_train)\n",
    "train_subset = generate_subset_data(dataset,train)\n",
    "val_subset = generate_subset_data(dataset,val)\n",
    "test_subset = generate_subset_data(dataset,test)\n",
    "\n",
    "# sanity checks\n",
    "\n",
    "print(f\"{len(whole_train_subset['data'])=}\")\n",
    "print(f\"{len(train_subset['data'])=}\")\n",
    "print(f\"{len(val_subset['data'])=}\")\n",
    "print(f\"{len(test_subset['data'])=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67a98f-9565-40ee-a357-5a2816643400",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d00451-4c26-402b-92b6-380d3f278f98",
   "metadata": {},
   "source": [
    "## prepare save path and file suffix\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Don't forget, the folder which the files will be saved is defined in .env file!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dacac81e-de6c-42d2-a678-b3bc448c8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file suffix with metadata\n",
    "params = {'train':train_size,\n",
    "          'eval':val_size,\n",
    "          'test':test_size,\n",
    "          'rseed':seed,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3276517d-6d93-4269-8b40-21001cff5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign filename to each subset\n",
    "names = {'whole_train_subset':whole_train_subset,\n",
    "         'train_subset':train_subset,\n",
    "         'val_subset':val_subset,\n",
    "         'test_subset':test_subset\n",
    "        }\n",
    "\n",
    "# Save (finally!)\n",
    "for name in names:\n",
    "    filepath = os.path.join(dataset_folder,dataset_name)\n",
    "    \n",
    "    if not os.path.isdir(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    \n",
    "    filename = os.path.join(filepath,name)\n",
    "    with open(filename+'.json','w') as fp:\n",
    "        json.dump(names[name],fp)\n",
    "\n",
    "with open(os.path.join(filepath,'metadata.json'),'w') as fp:\n",
    "    json.dump(params,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f0c3d-a737-4af0-b910-bee55c083d06",
   "metadata": {},
   "source": [
    "# Hurray!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (simao thesis)",
   "language": "python",
   "name": "simao_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
